{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eNdgsPRoOcyL",
        "TG6cIVHJ-TQM",
        "Lt1xoAw5-kQy",
        "etUoZc2yY0-Q",
        "Lux2XMbZY7LM",
        "banLOIX_Y8oi",
        "1Uny2dcrZFuS",
        "jJLdzYayZKT8",
        "mnUOSSgRgefo",
        "l4noWwsYiGxH",
        "LnBk-A6LaY4V",
        "nvw_eoaOZLkJ",
        "XKVzUWRJZNAE",
        "wZ9fZ3d1ZNzU",
        "d_iO47xsZPOS",
        "CcYFBFKQakTM",
        "TE19kzjnalIc"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hitesh-ag1/speech2text-analysis/blob/main/speech2text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Semantic Chunking of a YouTube Video** üìπ\n",
        "\n",
        "This part is divided into the following categories:\n",
        "1. Environment Setup üíª\n",
        "2. Downloading & Extracting Audio üîä\n",
        "3. Transcription üëÇ‚å®Ô∏è\n",
        "4. Time-alignment ‚åö\n",
        "5. Semantic Chunking ‚ö°‚Äã\n",
        "6. Future works üßë‚Äç‚öñÔ∏è"
      ],
      "metadata": {
        "id": "eNdgsPRoOcyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Environment Setup üíª"
      ],
      "metadata": {
        "id": "TG6cIVHJ-TQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube ffmpeg-python inflect yt-dlp openai-whisper pyannote.audio gdown"
      ],
      "metadata": {
        "id": "RSrlI4u_-S7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. visit hf.co/pyannote/speaker-diarization and accept user conditions\n",
        "# 2. visit hf.co/pyannote/segmentation and accept user conditions\n",
        "\n",
        "HF_ACCESS_TOKEN = \"\" # add HuggingFace access token to use pyannote for speaker diarization"
      ],
      "metadata": {
        "id": "VGqz6Qdq9-EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yt_dlp\n",
        "import ffmpeg\n",
        "import librosa\n",
        "import torchaudio\n",
        "import torch\n",
        "from IPython.display import Audio\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm\n",
        "import torchaudio.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torchaudio.transforms import Resample\n",
        "import string\n",
        "import inflect\n",
        "import re\n",
        "import pandas as pd\n",
        "import IPython\n",
        "import whisper\n",
        "import spacy\n",
        "from pyannote.audio import Pipeline\n",
        "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
        "import itertools\n",
        "import builtins\n",
        "import json\n",
        "import IPython"
      ],
      "metadata": {
        "id": "yDnzkRp7-Hqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "path = \"task1/\"\n",
        "os.mkdir(path)\n",
        "\n",
        "# Download pre-run results for t1\n",
        "# t1_results/t1_output.json is the final output file generated\n",
        "!gdown --id \"14dhcbALAIaiDSQitarRefqUcvZUEqoL8\" -O t1_results.zip\n",
        "!unzip t1_results.zip\n",
        "#path = \"t1_results/\""
      ],
      "metadata": {
        "id": "yajhIw76-NsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Download Video & Extract Audio üîä"
      ],
      "metadata": {
        "id": "Lt1xoAw5-kQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_youtube_video(url):\n",
        "    ydl_opts = {\n",
        "        'format': 'best',\n",
        "        'outtmpl': '%(title)s.%(ext)s',\n",
        "        'noplaylist': True,\n",
        "    }\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([url])"
      ],
      "metadata": {
        "id": "ANWfVBjC-O2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_youtube_video(\"https://www.youtube.com/watch?v=Sby1uJ_NFIY&ab_channel=moneycontrol\")\n",
        "os.rename(\"Sarvam AI Wants To Leverage AI In Health & Education Says Co Founder Vivek Raghavan With OpenHathi.mp4\", \"input.mp4\")\n",
        "ffmpeg.input(\"input.mp4\").output(path+\"input.wav\").run()"
      ],
      "metadata": {
        "id": "G1o_FJAl-Oyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path = \"drive/MyDrive/sarvam_assignment/task1/\"\n",
        "input_file = path+\"input.wav\"\n",
        "y, sr = librosa.load(input_file, sr=16000)"
      ],
      "metadata": {
        "id": "UZ0f1RM2F6Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Transcription of audio üëÇ‚å®Ô∏è‚Äã"
      ],
      "metadata": {
        "id": "nWjJSr0e-vrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I divided the audio into several batches of varying length based on silence detection. This batching was necessary for my speech-time alignment model which ran out of memory if I ran it in one go.\n",
        "\n",
        "Initially I used fixed-size batches (eg. 10s), however, this approach introduced transcription errors when there was an audio cut in the middle of a word."
      ],
      "metadata": {
        "id": "Y3PNyx0z3zMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_silence(audio_path, threshold_db=-30, min_silence_len=0.5, sampling_rate=16000):\n",
        "    y, sr = librosa.load(audio_path, sr=sampling_rate)\n",
        "    amplitude_envelope = librosa.feature.rms(y=y)[0]\n",
        "    amplitude_db = librosa.amplitude_to_db(amplitude_envelope, ref=np.max)\n",
        "    silent_frames = amplitude_db < threshold_db\n",
        "    frame_times = librosa.frames_to_time(np.arange(len(amplitude_db)), sr=sr)\n",
        "    silent_intervals = []\n",
        "    start_time = None\n",
        "    for i, silent in enumerate(silent_frames):\n",
        "        if silent and start_time is None:\n",
        "            start_time = frame_times[i]\n",
        "        elif not silent and start_time is not None:\n",
        "            end_time = frame_times[i]\n",
        "            if end_time - start_time >= min_silence_len:\n",
        "                silent_intervals.append((start_time, end_time))\n",
        "            start_time = None\n",
        "\n",
        "    if start_time is not None:\n",
        "        end_time = frame_times[-1]\n",
        "        if end_time - start_time >= min_silence_len:\n",
        "            silent_intervals.append((start_time, end_time))\n",
        "\n",
        "    return silent_intervals\n",
        "\n",
        "silent_intervals = detect_silence(input_file, threshold_db=-30, min_silence_len=1)"
      ],
      "metadata": {
        "id": "TGtBcdO7D0gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on literature, SeamlessM4T and Whisper are state-of-the-art open source models which can be used for ASR. I initially experimented with \"facebook/seamless-m4t-v2-large\" and \"openai/whisper-large-v3\" using huggingface library. Based on visual experimentation, Whisper performed significantly better than SeamlessM4T based on WER. Additionally, whisper is a much smaller model as compared to seamless model series. Hence, I eventually decided to continue with Whisper.\n",
        "\n",
        "Majority errors in transcription seemed to be around names or unclear audio (laughing, speakers overlap, diminished volume).\n",
        "\n",
        "In order to reduce errors around names, I use the \"initial_prompt\" option available in whisper which is only available through the openai library, and not using huggingface. Additionally, I added padding to the start and end of the audio to reduce errors in those particular timeframes."
      ],
      "metadata": {
        "id": "FK-wGjcqDalQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed-size batching, Huggingface library, SeamlessM4T model\n",
        "# Some experiments were done in Kaggle due to memory constraints in Colab\n",
        "\n",
        "# model_name = \"facebook/seamless-m4t-v2-large\"\n",
        "# model_name = \"openai/whisper-large-v3\"\n",
        "# processor = AutoProcessor.from_pretrained(model_name)\n",
        "# model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name).to(device)\n",
        "# chunk_size = 16000 * 10  # 10 seconds chunks\n",
        "# padding = np.zeros(int(2*sr)) # 2 seconds padding\n",
        "# padded_y = np.concatenate([padding, y, padding])\n",
        "# num_chunks = (len(padded_y) // chunk_size) + 1\n",
        "\n",
        "# transcriptions = []\n",
        "\n",
        "# for i in tqdm(range(num_chunks)):\n",
        "#     start_sample = i * chunk_size\n",
        "#     end_sample = min((i + 1) * chunk_size, len(padded_y))\n",
        "#     audio_chunk = padded_y[start_sample:end_sample]\n",
        "#     audio_inputs = processor(audio_chunk, return_tensors=\"pt\", sampling_rate=16000).to(device)\n",
        "#     with torch.no_grad():\n",
        "#         trans = model.generate(**audio_inputs).cpu().numpy()\n",
        "#     transcription = processor.decode(trans[0].tolist(), skip_special_tokens=True)\n",
        "#     transcriptions.append(transcription)\n"
      ],
      "metadata": {
        "id": "hBclZLIsEkTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_points = [(x1+x2)/2 for x1, x2 in silent_intervals]\n",
        "split_points.append(len(y)/sr)\n",
        "transcriptions = []\n",
        "model = whisper.load_model(\"large-v3\").to(device)\n",
        "prev = 0\n",
        "for i in tqdm(range(len(split_points))):\n",
        "    end = int(split_points[i]*sr)\n",
        "    padding = np.zeros(int(1*sr))\n",
        "    audio_chunk = y[prev:end]\n",
        "    audio_chunk = np.concatenate([padding, audio_chunk, padding])\n",
        "    prev = end\n",
        "    audio_chunk = torch.tensor(audio_chunk).to(torch.float32)\n",
        "    # transcription = model.transcribe(audio_chunk, language=\"en\", initial_prompt=\"Vocabulary: Sarvam, OpenHathi, Bhavish Agarwal, Krutrim, Pratyush, Sarvam.ai, NVIDIA, OpenHathi, Bala, Llama from Meta, Sarvam AI, OpenAI, Sarvam.\")\n",
        "    transcription = model.transcribe(audio_chunk, language=\"en\", initial_prompt=\"Vocabulary: Sarvam, OpenHathi, Bhavish Agarwal, Krutrim, Pratyush, Sarvam.ai, NVIDIA, OpenHathi, Bala, Llama 7 billion, Llama from Meta, Sarvam AI, OpenAI, Sarvam, Vivek Raghavan, NASA.\")\n",
        "    transcriptions.append(transcription['text'])"
      ],
      "metadata": {
        "id": "A6j7mxX8EZFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below code is used to free up GPU memory\n",
        "\n",
        "try:\n",
        "    del model\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    del waveform\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    del audio_chunk\n",
        "except:\n",
        "    pass\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "del gc"
      ],
      "metadata": {
        "id": "L3GOKaHZFXk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Time Align Transcript with Audio ‚åö‚Äã"
      ],
      "metadata": {
        "id": "nWHjOdGMFnvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to time align transcript with audio, I use Forced Alignment which aligns audio to transcript at a phoneme level. However, before I proceed I need to normalize my transcript by removing punctuations and converting numbers to words to make it input-ready to FA."
      ],
      "metadata": {
        "id": "U9J37rVl32Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = inflect.engine()\n",
        "def convert_numbers_to_words(text):\n",
        "    pattern = re.compile(r'(?<!\\d)(\\d+)(?!\\d)')\n",
        "    def replace_number(match):\n",
        "        number = match.group()\n",
        "        return p.number_to_words(number)\n",
        "    return pattern.sub(replace_number, text)\n",
        "\n",
        "proc_t = []\n",
        "for t in transcriptions:\n",
        "    t2 = t.lower()\n",
        "    common_replacements = {\n",
        "        '‚Äì': '',\n",
        "        \"‚Ä¶\": \"\"\n",
        "    }\n",
        "    for key, val in common_replacements.items():\n",
        "        t2 = t2.replace(key, val)\n",
        "\n",
        "    punctuation = string.punctuation.replace(\"'\", \"\")\n",
        "    translator = str.maketrans(punctuation, ' ' * len(punctuation), \"'\")\n",
        "    t2 = convert_numbers_to_words(t2).translate(translator).lower().strip().split()\n",
        "    proc_t.append(t2)"
      ],
      "metadata": {
        "id": "osQvTx8mFu-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Forced Aligner (FA) from the paper \"Scaling Speech Technology to 1,000+ Languages\" named MMS_FA which stands for \"Massively Multilingual Speech - Forced Aligner.\" Dictionary is used to tokenize the transcript so that it can be aligned with the output of the model. MMS_FA was used as it is a state-of-the-art aligner which is generalizable to multiple languages. Stepwise process for FA is highlighted below:\n",
        "\n",
        "\n",
        "\n",
        "1.   Generate emmissions - Utilise an acoustic model to generate framewise probability distribution over tokens.\n",
        "2.   Tokenize transcript\n",
        "3.   Compute alignments:\n",
        "  - Frame level\n",
        "  - Token level\n",
        "  - Word level\n",
        "\n"
      ],
      "metadata": {
        "id": "heNkDLwgJF5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bundle = torchaudio.pipelines.MMS_FA\n",
        "model = bundle.get_model(with_star=False).to(device)\n",
        "DICTIONARY = bundle.get_dict(star=None)"
      ],
      "metadata": {
        "id": "ngUPqV7OH3H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Returns timestamps of tokens and assosciated scores\n",
        "def align(emission, tokens):\n",
        "    targets = torch.tensor([tokens], dtype=torch.int32, device=device)\n",
        "    alignments, scores = F.forced_align(emission, targets, blank=0)\n",
        "\n",
        "    alignments, scores = alignments[0], scores[0]\n",
        "    scores = scores.exp()\n",
        "    return alignments, scores\n",
        "\n",
        "#Converts token-level alignment to word-level alignment\n",
        "def unflatten(list_, lengths):\n",
        "    assert len(list_) == sum(lengths)\n",
        "    i = 0\n",
        "    ret = []\n",
        "    for l in lengths:\n",
        "        ret.append(list_[i : i + l])\n",
        "        i += l\n",
        "    return ret"
      ],
      "metadata": {
        "id": "JfkNwa23KIPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prev = 0\n",
        "prevSize = 0\n",
        "aligned_speech = []\n",
        "for i in tqdm(range(len(split_points))):\n",
        "    end = int(split_points[i]*sr)\n",
        "    audio_chunk = y[prev:end]\n",
        "    prev = end\n",
        "    audio_chunk = torch.tensor([audio_chunk]).to(torch.float32).to(device)\n",
        "    with torch.inference_mode():\n",
        "        #generating emissions\n",
        "        emission, _ = model(audio_chunk)\n",
        "\n",
        "    #tokenising transcript\n",
        "    tokenized_transcript = [DICTIONARY[c.lower()] for word in proc_t[i] for c in word]\n",
        "\n",
        "    #generating frame level alignments\n",
        "    aligned_tokens, alignment_scores = align(emission, tokenized_transcript)\n",
        "\n",
        "    #generating token level alignments\n",
        "    token_spans = F.merge_tokens(aligned_tokens, alignment_scores)\n",
        "\n",
        "    #generating word level alignments\n",
        "    word_spans = unflatten(token_spans, [len(word) for word in proc_t[i]])\n",
        "    temp = []\n",
        "    for el in range(len(proc_t[i])):\n",
        "        ratio = audio_chunk.size(1) / emission.size(1)\n",
        "        x0 = int(ratio * word_spans[el][0].start) + prevSize\n",
        "        x1 = int(ratio * word_spans[el][-1].end) + prevSize\n",
        "        sc = []\n",
        "\n",
        "        #calculating average word level alignment score\n",
        "        for score in range(len(word_spans[el])):\n",
        "            sc.append(word_spans[el][score].score)\n",
        "        sc = np.sum(sc)/len(sc)\n",
        "\n",
        "        temp.append({\n",
        "            'word': proc_t[i][el],\n",
        "            'start':x0,\n",
        "            'stop':x1,\n",
        "            'score':sc\n",
        "        })\n",
        "    aligned_speech.append(temp)\n",
        "    prevSize += audio_chunk.size(1)"
      ],
      "metadata": {
        "id": "ehJZsYtUKTN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# free up GPU\n",
        "\n",
        "try:\n",
        "    del model\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    del waveform\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    del audio_chunk\n",
        "except:\n",
        "    pass\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "del gc"
      ],
      "metadata": {
        "id": "6HPsDBUIP3w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the task's focus metric is precision, I optimized for it by removing words with low alignment scores (<0.1). This threshold can be finetuned based on WER using ground truth labels."
      ],
      "metadata": {
        "id": "NiAVA7S2QJft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod_align_sp = []\n",
        "for el in aligned_speech:\n",
        "    temp_as = pd.DataFrame(el)\n",
        "    temp_as.loc[temp_as[temp_as['score']<0.1].index, 'word'] = '<REMOVED>'\n",
        "    mod_align_sp.append(temp_as.to_dict('records'))"
      ],
      "metadata": {
        "id": "PdsSigf6QHVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Semantic chunking ‚ö°‚Äã"
      ],
      "metadata": {
        "id": "keYbaAdSwIZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text-based:\n",
        "1. Split by sentences. Use \"spacy/en_core_web_sm\" to detect sentences and split at every new sentence.\n",
        "2. Split on conjunctions. Manual regex-based split whenever conjunctions like \"and\", \"or\", \"so\", etc. are detected.\n",
        "\n",
        "Voice-based:\n",
        "1. Split on silence (>0.5s). Already implemented.\n",
        "2. Split by speaker. Ideally, a single segment should have only one speaker and should be split as soon as next speaker starts speaking. This is implemented using pyannotate's speaker diarizer."
      ],
      "metadata": {
        "id": "AO7HJBvs35U2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence-based splitting"
      ],
      "metadata": {
        "id": "Xa7sPf6FyZq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "for i in transcriptions:\n",
        "    temp = (list(nlp(i).sents))\n",
        "    temp = [str(t).strip() for t in temp if len(str(t).strip())>0]\n",
        "    sentences.append(temp)"
      ],
      "metadata": {
        "id": "yphh5ADIwLxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_word_indices(sent, word):\n",
        "    sent_words = sent.split()\n",
        "    word_words = word.split()\n",
        "\n",
        "    sent_length = len(sent_words)\n",
        "\n",
        "    for i in range(len(word_words) - sent_length + 1):\n",
        "        if word_words[i:i + sent_length] == sent_words:\n",
        "            return i, i + sent_length - 1"
      ],
      "metadata": {
        "id": "wmciOkqx1P-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_align = []\n",
        "for sp in range(len(mod_align_sp)):\n",
        "    df = pd.DataFrame(mod_align_sp[sp])\n",
        "    words = ' '.join(list(df['word']))\n",
        "    punctuation = string.punctuation.replace(\"'\", \"\")\n",
        "    translator = str.maketrans(punctuation, ' ' * len(punctuation), \"'\")\n",
        "    for i in range(len(sentences[sp])):\n",
        "        sent = ' '.join(convert_numbers_to_words(str(sentences[sp][i])).translate(translator).lower().strip().split())\n",
        "        sent2 = re.escape(sent)\n",
        "        sent2 = rf'\\b{sent}\\b'\n",
        "\n",
        "        if bool(re.search(sent2, words)):\n",
        "            start_index, end_index = find_word_indices(sent, words)\n",
        "            li = list((df[start_index:end_index+1]).to_records())\n",
        "            sentence_align.append([str(sentences[sp][i]), li[0]['start']/sr, li[-1]['stop']/sr, li])\n",
        "\n",
        "sentence_align = pd.DataFrame(sentence_align, columns=['sentence', 'start', 'stop', 'ts'])\n",
        "sentence_align = sentence_align.drop(list(sentence_align[sentence_align.duplicated('ts')].index))\n",
        "sentence_align.index = np.arange(len(sentence_align))\n",
        "sentence_align.to_csv(path+\"sentence_alignment.csv\")"
      ],
      "metadata": {
        "id": "-VDixpLBxpLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence alignment is saved in a CSV file here."
      ],
      "metadata": {
        "id": "yAEc32KtC3bR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Speaker Diarization"
      ],
      "metadata": {
        "id": "9hHVoPTPybq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy of speaker diarization is very poor**. In the future, I would finetune the diarization model using the [diarizers](https://github.com/huggingface/diarizers) library to improve the accuracy."
      ],
      "metadata": {
        "id": "Y5tbm_gj1lKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since Speaker diarization is extremely slow and consumes limited colab GPU time \\\n",
        "# I only ran it once and saved the results\n",
        "\n",
        "pipeline = Pipeline.from_pretrained(\n",
        "    \"pyannote/speaker-diarization-3.1\",\n",
        "    use_auth_token=HF_ACCESS_TOKEN) # remember to use your HF access token & agree to pyannotate's T&C\n",
        "pipeline.to(torch.device(\"cuda\"))\n",
        "\n",
        "with ProgressHook() as hook:\n",
        "    diarization = pipeline(input_file, hook=hook)\n",
        "\n",
        "with open(\"audio_diar.rttm\", \"w\") as rttm:\n",
        "    diarization.write_rttm(rttm)\n",
        "\n",
        "diar_labels = []\n",
        "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "    diar_labels.append([speaker, turn.start, turn.end])\n",
        "\n",
        "def merge_diarization(diarization_list):\n",
        "    if not diarization_list:\n",
        "        return []\n",
        "\n",
        "    merged_list = [diarization_list[0]]\n",
        "\n",
        "    for current in diarization_list[1:]:\n",
        "        previous = merged_list[-1]\n",
        "        if current[0] == previous[0]:\n",
        "            merged_list[-1] = [previous[0], previous[1], current[2]]\n",
        "        else:\n",
        "            merged_list.append(current)\n",
        "\n",
        "    return merged_list\n",
        "\n",
        "diar_labels2 = merge_diarization(diar_labels)\n",
        "diar_labels3 = []\n",
        "for current in diar_labels2:\n",
        "    if current[2]-current[1]>2:\n",
        "        diar_labels3.append(current)\n",
        "pd.DataFrame(diar_labels3).to_csv(path+\"speaker_alignment.csv\")"
      ],
      "metadata": {
        "id": "_YWQ47eF08Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result of speaker alignment is saved in a CSV file."
      ],
      "metadata": {
        "id": "xFmJkVoADB8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge speaker diarization & sentence split"
      ],
      "metadata": {
        "id": "wdc2-nQ62pD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: Since there are different variations of whisper results in each run & I only ran speaker diarization once, I am loading the results from that particular experiment so that there are no discrepancies between the two outputs.**"
      ],
      "metadata": {
        "id": "06qnk7-GHniC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"t1_results/\"\n",
        "df1 = pd.read_csv(path+\"speaker_alignment.csv\")\n",
        "df2 = pd.read_csv(path+\"sentence_alignment.csv\")\n",
        "\n",
        "# df1 = pd.DataFrame(diar_labels3)\n",
        "# df2 = sentence_align"
      ],
      "metadata": {
        "id": "Qm6v6fb63Bz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns = ['del', 'speaker', 'start', 'end']\n",
        "df2.columns = ['del', 'sentence', 'start', 'end', 'ts']\n",
        "df2['ts'] = df2['ts'].apply(eval)\n",
        "df1 = df1.drop('del', axis=1)\n",
        "df2 = df2.drop('del', axis=1)\n",
        "\n",
        "df2['mean'] = (df2['start']+df2['end'])/2\n",
        "speaker = np.full(len(df2), '', dtype=object)\n",
        "for i, row in df1.iterrows():\n",
        "    start, stop = row['start'], row['end']\n",
        "    speaker[df2[(df2['mean']<stop) & (df2['mean']>start)].index] = row['speaker']\n",
        "df2['speaker'] = speaker\n",
        "df2 = df2.drop('mean', axis=1)\n",
        "df2['duration'] = df2['end'] - df2['start']\n",
        "\n",
        "for i, row in df2[df2['speaker']==''].iterrows():\n",
        "  start, stop = row['start'], row['end']\n",
        "  try:\n",
        "    speaker = df1[((df1['start']<start) & (start<df1['end'])) | ((df1['end']>stop) & (stop>df1['start']))].iloc[-1]['speaker']\n",
        "    df2.loc[i,'speaker'] = speaker\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "df2['speaker'] = df2['speaker'].replace('', pd.NA).ffill()\n",
        "df2 = df2.sort_values(by='start')\n",
        "df2.index = np.arange(len(df2))"
      ],
      "metadata": {
        "id": "BfE3w7fA2sBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = inflect.engine()\n",
        "def convert_numbers_to_words(text):\n",
        "    pattern = re.compile(r'(?<!\\d)(\\d+)(?!\\d)')\n",
        "    def replace_number(match):\n",
        "        number = match.group()\n",
        "        return p.number_to_words(number)\n",
        "    return pattern.sub(replace_number, text)\n",
        "\n",
        "punctuation = string.punctuation.replace(\"'\", \"\")\n",
        "translator = str.maketrans(punctuation, ' ' * len(punctuation), \"'\")"
      ],
      "metadata": {
        "id": "tQ9VsSqY3GPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split by conjunction"
      ],
      "metadata": {
        "id": "IOO_rD1t3IsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sentence_by_connector_and_time(row, max_duration=15):\n",
        "      split_flag = False\n",
        "      if row['duration']>15:\n",
        "        split_flag = True\n",
        "\n",
        "      connectors = [\n",
        "    \"and\", \"or\", \"but\", \"nor\", \"for\", \"yet\", \"so\", \"because\", \"although\", \"if\", \"since\", \"unless\", \"while\",\n",
        "    \"however\", \"therefore\", \"moreover\", \"nevertheless\", \"furthermore\", \"consequently\"\n",
        "      ]\n",
        "\n",
        "      sentences = []\n",
        "      sentences2 = []\n",
        "      timestamps = []\n",
        "      current_ts = []\n",
        "      current_sentence = []\n",
        "      current_sentence2 = []\n",
        "      duration = []\n",
        "      speaker=[]\n",
        "      start = []\n",
        "      end = []\n",
        "      for i in range(len(row['ts'])):\n",
        "          curr_speaker = row['speaker']\n",
        "          word = row['ts'][i][1]\n",
        "          try:\n",
        "            word_sentence = row['sentence'].split()[i]\n",
        "          except:\n",
        "            word_sentence = \"\"\n",
        "          if (word in connectors and split_flag and i>0):\n",
        "              sentences.append(' '.join(current_sentence))\n",
        "              timestamps.append(current_ts)\n",
        "              start.append(current_ts[0][2]/16000)\n",
        "              end.append(current_ts[-1][3]/16000)\n",
        "              duration.append(end[-1]-start[-1])\n",
        "              speaker.append(row['speaker'])\n",
        "              current_sentence = [word]\n",
        "              current_ts = [row['ts'][i]]\n",
        "          else:\n",
        "              current_sentence.append(word)\n",
        "              current_ts.append(row['ts'][i])\n",
        "              # duration.append((current_ts[-1][3]-current_ts[0][2])/16000)\n",
        "          if (word_sentence.lower() in connectors and i>0 and split_flag):\n",
        "              sentences2.append(' '.join(current_sentence2))\n",
        "              current_sentence2 = [word_sentence]\n",
        "          else:\n",
        "              current_sentence2.append(word_sentence)\n",
        "\n",
        "      if current_sentence:\n",
        "          sentences.append(' '.join(current_sentence))\n",
        "          timestamps.append(current_ts)\n",
        "          start.append(current_ts[0][2]/16000)\n",
        "          end.append(current_ts[-1][3]/16000)\n",
        "          duration.append(end[-1]-start[-1])\n",
        "          speaker.append(curr_speaker)\n",
        "\n",
        "      current_sentence2.append(' '.join(row['sentence'].split()[i+1:]))\n",
        "      if current_sentence2:\n",
        "          sentences2.append(' '.join(current_sentence2).strip())\n",
        "\n",
        "      return [sentences, sentences2, timestamps, duration, speaker, start, end]\n",
        "\n",
        "new_df = []\n",
        "for i, row in df2.iterrows():\n",
        "    proc = split_sentence_by_connector_and_time(row)\n",
        "    for j in range(len(proc[0])):\n",
        "      new_df.append([proc[0][j], proc[1][j], proc[2][j], proc[3][j], proc[4][j], proc[5][j], proc[6][j]])\n",
        "new_df = pd.DataFrame(new_df, columns=['sentence', 'sentences2' ,'ts', 'duration', 'speaker', 'start', 'end'])"
      ],
      "metadata": {
        "id": "GNRVfmyQ3OZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = new_df.copy()\n",
        "\n",
        "merged_rows = []\n",
        "current_row = df.iloc[0]\n",
        "\n",
        "for i in range(1, len(df)):\n",
        "    next_row = df.iloc[i]\n",
        "    if current_row['speaker'] == next_row['speaker'] and (next_row['end'] - current_row['start']) < 15 and (current_row['end']==next_row['start']):\n",
        "        current_row['sentence'] += \" \" + next_row['sentence']\n",
        "        current_row['sentences2'] += \" \" + next_row['sentences2']\n",
        "        current_row['end'] = next_row['end']\n",
        "        current_row['duration'] = next_row['end'] - current_row['start']\n",
        "        current_row['ts'].extend(next_row['ts'])\n",
        "    else:\n",
        "        merged_rows.append(current_row)\n",
        "        current_row = next_row\n",
        "\n",
        "merged_rows.append(current_row)\n",
        "merged_df = pd.DataFrame(merged_rows)\n",
        "merged_df = merged_df.drop_duplicates('sentence')\n",
        "merged_df = merged_df[merged_df['duration']>1]\n",
        "merged_df.index = np.arange(len(merged_df))"
      ],
      "metadata": {
        "id": "8QegKN2N3cuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = [\n",
        "    {\n",
        "        \"chunk_id\": i,\n",
        "        \"chunk_length\": round(row[\"duration\"], 1),\n",
        "        \"text\": row[\"sentences2\"],\n",
        "        \"start_time\": round(row[\"start\"], 1),\n",
        "        \"end_time\": round(row[\"end\"], 1)\n",
        "    }\n",
        "    for i, row in merged_df.iterrows()\n",
        "]"
      ],
      "metadata": {
        "id": "_iTzt0jH3f3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"t1_output.json\", \"w\") as file:\n",
        "    json.dump(chunks, file, indent=4)"
      ],
      "metadata": {
        "id": "aQ4bW63fLAK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks"
      ],
      "metadata": {
        "id": "l3uwCwh_NyXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Future works üßë‚Äç‚öñÔ∏è"
      ],
      "metadata": {
        "id": "MimYoJwE4qmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code written above is very task specific and I would focus on improving generalizability of the code like adding multilingual support and making it scalable for even longer videos. The current output might have small chunks as well which maybe less than 3-4s. When creating a dataset for ASR training, it is generally advisable to have chunks >5s. If that was the intentended use of the chunks, I would focus on min. time of chunks as well and merging of multiple chunks according to time.\n",
        "\n",
        "Additionally, there was no metric calculation and score optimization on WER. I would transcribe the segments using a paid ASR solution like Azure and compare those as ground truth with my transcription. This would help optimize the above code for better precision. Optimization would be done on prompt to whisper, word score alignment threshold, choice of open-source models like Whisper and MMS_FA."
      ],
      "metadata": {
        "id": "jQ9aV_tc40sU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Exploratory Data Analysis of New Testament Audio and Text üìñ\n",
        "\n",
        "This task's solution is divided into the following categories:\n",
        "1. Environment Setup üíª\n",
        "2. Web scraping üï∏Ô∏è\n",
        "3. Data Preparation ü§ó\n",
        "4. Exploratory Data Analysis üõ≥Ô∏è:\n",
        "  - üîä‚å®Ô∏è Speech-Text Alignment\n",
        "  - üîä‚å®Ô∏è Word-Level Speech-Text Analysis\n",
        "  - üîä Audio-Based Anomaly Detection\n",
        "  - üîä Audio Duration Analysis\n",
        "  - üîä Audio Quality Analysis\n",
        "  - ‚å®Ô∏è Word-Level Text Analysis\n",
        "  - ‚å®Ô∏è Text Bias Analysis\n",
        "  - ‚å®Ô∏è Phoneme-level Analysis\n",
        "5. Conclusion üìÑ"
      ],
      "metadata": {
        "id": "AoWPehYiV9d0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Environment Setup üíª"
      ],
      "metadata": {
        "id": "etUoZc2yY0-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium google-colab-selenium ffmpeg-python Unidecode uroman-python huggingface_hub fastdtw phonemizer fastdtw datasets"
      ],
      "metadata": {
        "id": "g_amHoQZY6t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/isi-nlp/uroman.git"
      ],
      "metadata": {
        "id": "i4yshwQtY6qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y festival espeak-ng mbrola"
      ],
      "metadata": {
        "id": "TunoqEsPbUEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "import requests\n",
        "import google_colab_selenium as gs\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import os\n",
        "import ffmpeg\n",
        "import librosa\n",
        "import torchaudio\n",
        "import torch\n",
        "from IPython.display import Audio\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm\n",
        "import torchaudio.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "import IPython\n",
        "import spacy\n",
        "import itertools\n",
        "import builtins\n",
        "import json\n",
        "import IPython\n",
        "import unicodedata\n",
        "from unidecode import unidecode\n",
        "import tempfile\n",
        "import datasets\n",
        "from datasets import Dataset, Audio, load_dataset\n",
        "from fastdtw import fastdtw\n",
        "from scipy.fftpack import fft\n",
        "from sklearn.decomposition import PCA\n",
        "from torchaudio import transforms\n",
        "import nltk\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from huggingface_hub import login\n",
        "from phonemizer import phonemize\n",
        "from phonemizer.backend import EspeakBackend\n",
        "from phonemizer.separator import Separator\n",
        "from collections import Counter\n",
        "from selenium.webdriver.common.by import By\n",
        "import seaborn as sns\n",
        "from datasets import DatasetDict\n",
        "import plotly.offline as py\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls\n",
        "py.init_notebook_mode(connected=True)\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Hf6IwubKbjEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id \"1a78LH4cfSw1XPzBT_XVEgzFE9vg97b5T\" -O bible/punctuations.lst"
      ],
      "metadata": {
        "id": "HjQT3zFtW_xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import login\n",
        "# HF_ACCESS_TOKEN = \"\" # add your HF token\n",
        "# login(HF_ACCESS_TOKEN)"
      ],
      "metadata": {
        "id": "Qq_Xh8ujdq5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Web scraping üï∏Ô∏è"
      ],
      "metadata": {
        "id": "Lux2XMbZY7LM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the below web scraping script, I have added appropriate error handling techniques to make the below script scalable. Some important elements:\n",
        "- Wait for 5s before sending next request.\n",
        "- Use WebDriverWait so that the script waits for the website to completely load until the described element appears.\n",
        "- If there is any error, it waits for 20s and retries with appropriate logging\n",
        "- If the extracted text is less than 2 characters, wait for 20s and retry with logging.\n",
        "- If size of audio file is too small, retry after 20s with logging."
      ],
      "metadata": {
        "id": "NSQLpAF8e3G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_text_audio(count, book, chapter):\n",
        "  url = 'https://live.bible.is/bible/HINOHC/'+book+\"/\"+chapter\n",
        "  driver.get(url)\n",
        "  try:\n",
        "\n",
        "    video_tag = WebDriverWait(driver, 25).until(\n",
        "          EC.presence_of_element_located((By.CSS_SELECTOR, 'video.audio-player'))\n",
        "    )\n",
        "    text_tag = WebDriverWait(driver, 25).until(\n",
        "          EC.presence_of_element_located((By.CSS_SELECTOR, 'div.chapter.justify'))\n",
        "    )\n",
        "  except:\n",
        "    print(\"Something is wrong. Retrying chapter \"+ count+\" after 20s\")\n",
        "    time.sleep(20)\n",
        "    return download_text_audio(count, book, chapter)\n",
        "\n",
        "  # video_tag = driver.presence_of_element_located((By.CSS_SELECTOR, 'video.audio-player'))\n",
        "  # text_tag = driver.find_element((By.CSS_SELECTOR, 'div.chapter.justify'))\n",
        "  chapter_html = text_tag.get_attribute('outerHTML')\n",
        "  soup = BeautifulSoup(chapter_html, 'html.parser')\n",
        "\n",
        "  verses = soup.find_all('span', class_='align-left')\n",
        "  verse_texts = [verse.get_text(separator=\" \").split('\\xa0')[2].strip() for verse in verses]\n",
        "  if len(verse_texts)<2:\n",
        "    print(\"Verse too small. Retrying chapter \"+ count+\" after 20s\")\n",
        "    time.sleep(20)\n",
        "    return download_text_audio(count, book, chapter)\n",
        "\n",
        "  audio_url = video_tag.get_attribute('src')\n",
        "  # print(f'{count}. Audio URL: {audio_url}')\n",
        "\n",
        "  audio_response = requests.get(audio_url)\n",
        "  audio_file_path = base_dir+'audio/'+'audio_'+count+'.mp3'\n",
        "  with open(audio_file_path, 'wb') as file:\n",
        "      file.write(audio_response.content)\n",
        "  if (os.path.getsize(audio_file_path) / 1024) < 500:\n",
        "    print(\"Audio file too small. Retrying chapter \"+ count+\" after 20s\")\n",
        "    time.sleep(20)\n",
        "    return download_text_audio(count, book, chapter)\n",
        "\n",
        "  return verse_texts\n"
      ],
      "metadata": {
        "id": "dpJl9IlFY8O4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver = gs.Chrome()\n",
        "url = 'https://live.bible.is/bible/HINOHC/GEN/1'\n",
        "driver.get(url)\n",
        "driver.implicitly_wait(10)\n",
        "\n",
        "script_tag = driver.find_element(By.XPATH, '//script[@id=\"__NEXT_DATA__\"]')\n",
        "json_data = script_tag.get_attribute('innerHTML')\n",
        "\n",
        "data = json.loads(json_data)\n",
        "chapter_text_list = data[\"props\"][\"pageProps\"][\"chapterText\"]\n",
        "chapters = {}\n",
        "for el in data[\"props\"][\"pageProps\"][\"books\"]:\n",
        "  if el['testament']=='NT':\n",
        "    chapters[el['testament_order']] = [el['book_id'], el['chapters']]\n",
        "\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "mnc0IsmcZk9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chapters[1])"
      ],
      "metadata": {
        "id": "6nUPKBqqaaNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"bible/\"\n",
        "audio_dir = base_dir+\"audio/\"\n",
        "txt_dir = base_dir+\"text/\"\n",
        "os.mkdir(base_dir)\n",
        "os.mkdir(audio_dir)\n",
        "os.mkdir(txt_dir)"
      ],
      "metadata": {
        "id": "gdag_iDyaho2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ran this function only once. The printed output is just for few initial chapters and then was interrupted.\n",
        "# However, dataset in HF contains all chapters\n",
        "\n",
        "base_url = \"https://live.bible.is/bible/HINOHC/\"\n",
        "chapter=1\n",
        "driver = gs.Chrome()\n",
        "ch_texts = {}\n",
        "for key, val in chapters.items():\n",
        "  i = str(chapters[key][0])\n",
        "  for j in chapters[key][1]:\n",
        "    ch_texts[chapter] = download_text_audio(str(chapter), i, str(j))\n",
        "    print(\"Downloaded \"+str(chapter)+\"/260\")\n",
        "    chapter+=1\n",
        "    time.sleep(5)\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "OPUg9BA2Y7pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, val in ch_texts.items():\n",
        "  with open(base_dir+\"text\"+\"/\"+\"text_\"+str(key)+\".txt\", \"w\") as f:\n",
        "    for line in val:\n",
        "      f.write(line + '\\n')"
      ],
      "metadata": {
        "id": "f49n8APrb9UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Data Preparation ü§ó"
      ],
      "metadata": {
        "id": "banLOIX_Y8oi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add dataset to HF Hub for easy and efficient retrieval"
      ],
      "metadata": {
        "id": "jr3dtvx4eQmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = base_dir\n",
        "audio_path = audio_dir\n",
        "text_path = txt_dir\n",
        "\n",
        "records = []\n",
        "\n",
        "audio_files = sorted(os.listdir(audio_path))\n",
        "text_files = sorted(os.listdir(text_path))\n",
        "\n",
        "for audio_file, text_file in zip(audio_files, text_files):\n",
        "    if audio_file.endswith(\".mp3\") and text_file.endswith(\".txt\"):\n",
        "        audio_file_path = os.path.join(audio_path, audio_file)\n",
        "        text_file_path = os.path.join(text_path, text_file)\n",
        "\n",
        "        with open(text_file_path, \"r\") as f:\n",
        "            text = f.read().strip()\n",
        "\n",
        "        records.append({\"audio\": audio_file_path, \"text\": text})\n",
        "\n",
        "\n",
        "dataset = Dataset.from_dict({\"audio\": [e[\"audio\"] for e in records], \"text\": [e[\"text\"] for e in records]})\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "dataset_dict = DatasetDict({\"dataset\": dataset})\n",
        "dataset_dict.save_to_disk(\"/bible_hf_dataset\")\n",
        "# dataset_dict.push_to_hub(\"hiteshagarwal/bible_stt\") # Uncomment if want to push data"
      ],
      "metadata": {
        "id": "7rY-rLYbY9kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Exploratory Data Analysis üõ≥Ô∏è"
      ],
      "metadata": {
        "id": "1Uny2dcrZFuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"hiteshagarwal/bible_stt\")\n",
        "dataset = dataset['dataset']\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "path = \"bible/\""
      ],
      "metadata": {
        "id": "H4DLCZZagcgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4.1: Speech-Text Alignment"
      ],
      "metadata": {
        "id": "jJLdzYayZKT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will follow a similar alignment approach as Task 1 with difference in:\n",
        "1. Normalization - Hindi specific normalization techniques to remove punctuations.\n",
        "2. Uromanization - Converts Devanagri to Latin which is interpretable by MMS_FA model. Currently, I have used uromanization for transliteration due to best accuracy as compared to other transliteration models. However, one major drawback with uromanization is that it always converts Bindu matra to letter 'm' which introduces error.\n",
        "3. Add star (*) token in the beginning of every text for each audio file in case the paired text does not cover the beginning of the audio which is the case in most audio files. Enable star token in MMS_FA model."
      ],
      "metadata": {
        "id": "BG3fNSrjgyJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Functions for normalization & uromanization"
      ],
      "metadata": {
        "id": "mnUOSSgRgefo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colon = \":\"\n",
        "comma = \",\"\n",
        "exclamation_mark = \"!\"\n",
        "period = re.escape(\".\")\n",
        "question_mark = re.escape(\"?\")\n",
        "semicolon = \";\"\n",
        "\n",
        "left_curly_bracket = \"{\"\n",
        "right_curly_bracket = \"}\"\n",
        "quotation_mark = '\"'\n",
        "\n",
        "basic_punc = (\n",
        "    period\n",
        "    + question_mark\n",
        "    + comma\n",
        "    + colon\n",
        "    + exclamation_mark\n",
        "    + left_curly_bracket\n",
        "    + right_curly_bracket\n",
        ")\n",
        "\n",
        "# General punc unicode block (0x2000-0x206F)\n",
        "zero_width_space = r\"\\u200B\"\n",
        "zero_width_nonjoiner = r\"\\u200C\"\n",
        "left_to_right_mark = r\"\\u200E\"\n",
        "right_to_left_mark = r\"\\u200F\"\n",
        "left_to_right_embedding = r\"\\u202A\"\n",
        "pop_directional_formatting = r\"\\u202C\"\n",
        "\n",
        "# Here are some commonly ill-typed versions of apostrophe\n",
        "right_single_quotation_mark = r\"\\u2019\"\n",
        "left_single_quotation_mark = r\"\\u2018\"\n",
        "\n",
        "# Language specific definitions\n",
        "# Hindi\n",
        "hindi_danda = u\"\\u0964\"\n",
        "\n",
        "lesser_than_symbol = r\"&lt;\"\n",
        "greater_than_symbol = r\"&gt;\"\n",
        "\n",
        "lesser_than_sign = r\"\\u003c\"\n",
        "greater_than_sign = r\"\\u003e\"\n",
        "\n",
        "nbsp_written_form = r\"&nbsp\"\n",
        "\n",
        "# Quotation marks\n",
        "left_double_quotes = r\"\\u201c\"\n",
        "right_double_quotes = r\"\\u201d\"\n",
        "left_double_angle = r\"\\u00ab\"\n",
        "right_double_angle = r\"\\u00bb\"\n",
        "left_single_angle = r\"\\u2039\"\n",
        "right_single_angle = r\"\\u203a\"\n",
        "low_double_quotes = r\"\\u201e\"\n",
        "low_single_quotes = r\"\\u201a\"\n",
        "high_double_quotes = r\"\\u201f\"\n",
        "high_single_quotes = r\"\\u201b\"\n",
        "\n",
        "all_punct_quotes = (\n",
        "    left_double_quotes\n",
        "    + right_double_quotes\n",
        "    + left_double_angle\n",
        "    + right_double_angle\n",
        "    + left_single_angle\n",
        "    + right_single_angle\n",
        "    + low_double_quotes\n",
        "    + low_single_quotes\n",
        "    + high_double_quotes\n",
        "    + high_single_quotes\n",
        "    + right_single_quotation_mark\n",
        "    + left_single_quotation_mark\n",
        ")\n",
        "mapping_quotes = (\n",
        "    \"[\"\n",
        "    + high_single_quotes\n",
        "    + right_single_quotation_mark\n",
        "    + left_single_quotation_mark\n",
        "    + \"]\"\n",
        ")\n",
        "\n",
        "\n",
        "# Digits\n",
        "\n",
        "english_digits = r\"\\u0030-\\u0039\"\n",
        "bengali_digits = r\"\\u09e6-\\u09ef\"\n",
        "khmer_digits = r\"\\u17e0-\\u17e9\"\n",
        "devanagari_digits = r\"\\u0966-\\u096f\"\n",
        "oriya_digits = r\"\\u0b66-\\u0b6f\"\n",
        "extended_arabic_indic_digits = r\"\\u06f0-\\u06f9\"\n",
        "kayah_li_digits = r\"\\ua900-\\ua909\"\n",
        "fullwidth_digits = r\"\\uff10-\\uff19\"\n",
        "malayam_digits = r\"\\u0d66-\\u0d6f\"\n",
        "myanmar_digits = r\"\\u1040-\\u1049\"\n",
        "roman_numeral = r\"\\u2170-\\u2179\"\n",
        "nominal_digit_shapes = r\"\\u206f\"\n",
        "\n",
        "# Load punctuations from MMS-lab data\n",
        "with open(f\"{path}/punctuations.lst\", \"r\") as punc_f:\n",
        "    punc_list = punc_f.readlines()\n",
        "\n",
        "punct_pattern = r\"\"\n",
        "for punc in punc_list:\n",
        "    # the first character in the tab separated line is the punc to be removed\n",
        "    punct_pattern += re.escape(punc.split(\"\\t\")[0])\n",
        "\n",
        "shared_digits = (\n",
        "    english_digits\n",
        "    + bengali_digits\n",
        "    + khmer_digits\n",
        "    + devanagari_digits\n",
        "    + oriya_digits\n",
        "    + extended_arabic_indic_digits\n",
        "    + kayah_li_digits\n",
        "    + fullwidth_digits\n",
        "    + malayam_digits\n",
        "    + myanmar_digits\n",
        "    + roman_numeral\n",
        "    + nominal_digit_shapes\n",
        ")\n",
        "\n",
        "shared_punc_list = (\n",
        "    basic_punc\n",
        "    + all_punct_quotes\n",
        "    + greater_than_sign\n",
        "    + lesser_than_sign\n",
        "    + semicolon\n",
        "    + hindi_danda\n",
        "    + quotation_mark\n",
        "    + punct_pattern\n",
        "\n",
        ")\n",
        "\n",
        "shared_mappping = {\n",
        "    lesser_than_symbol: \"\",\n",
        "    greater_than_symbol: \"\",\n",
        "    nbsp_written_form: \"\",\n",
        "    r\"(\\S+)\" + mapping_quotes + r\"(\\S+)\": r\"\\1'\\2\",\n",
        "}\n",
        "\n",
        "shared_deletion_list = (\n",
        "    left_to_right_mark\n",
        "    + zero_width_nonjoiner\n",
        "    + zero_width_space\n",
        "    + pop_directional_formatting\n",
        "    + right_to_left_mark\n",
        "    + left_to_right_embedding\n",
        ")\n",
        "\n",
        "norm_config = {\n",
        "    \"*\": {\n",
        "        \"lower_case\": True,\n",
        "        \"punc_set\": shared_punc_list,\n",
        "        \"del_set\": shared_deletion_list,\n",
        "        \"mapping\": shared_mappping,\n",
        "        \"digit_set\": shared_digits,\n",
        "        \"unicode_norm\": \"NFKC\",\n",
        "        \"rm_diacritics\" : False,\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "def text_normalize(text, iso_code=\"hi\", lower_case=True, remove_numbers=True, remove_brackets=True):\n",
        "\n",
        "    config = norm_config.get(iso_code, norm_config[\"*\"])\n",
        "\n",
        "    for field in [\"lower_case\", \"punc_set\",\"del_set\", \"mapping\", \"digit_set\", \"unicode_norm\"]:\n",
        "        if field not in config:\n",
        "            config[field] = norm_config[\"*\"][field]\n",
        "\n",
        "\n",
        "    text = unicodedata.normalize(config[\"unicode_norm\"], text)\n",
        "\n",
        "    # Convert to lower case\n",
        "\n",
        "    if config[\"lower_case\"] and lower_case:\n",
        "        text = text.lower()\n",
        "\n",
        "    # brackets\n",
        "\n",
        "    # always text inside brackets with numbers in them. Usually corresponds to \"(Sam 23:17)\"\n",
        "    text = re.sub(r\"\\([^\\)]*\\d[^\\)]*\\)\", \" \", text)\n",
        "    if remove_brackets:\n",
        "        text = re.sub(r\"\\([^\\)]*\\)\", \" \", text)\n",
        "\n",
        "    # Apply mappings\n",
        "\n",
        "    for old, new in config[\"mapping\"].items():\n",
        "        text = re.sub(old, new, text)\n",
        "\n",
        "    # Replace punctutations with space\n",
        "\n",
        "    punct_pattern = r\"[\" + config[\"punc_set\"]\n",
        "\n",
        "    punct_pattern += \"]\"\n",
        "\n",
        "    normalized_text = re.sub(punct_pattern, \" \", text)\n",
        "\n",
        "    # remove characters in delete list\n",
        "\n",
        "    delete_patten = r\"[\" + config[\"del_set\"] + \"]\"\n",
        "\n",
        "    normalized_text = re.sub(delete_patten, \"\", normalized_text)\n",
        "\n",
        "    # Remove words containing only digits\n",
        "    # We check for 3 cases  a)text starts with a number b) a number is present somewhere in the middle of the text c) the text ends with a number\n",
        "    # For each case we use lookaround regex pattern to see if the digit pattern in preceded and followed by whitespaces, only then we replace the numbers with space\n",
        "    # The lookaround enables overlapping pattern matches to be replaced\n",
        "\n",
        "    if remove_numbers:\n",
        "\n",
        "        digits_pattern = \"[\" + config[\"digit_set\"]\n",
        "\n",
        "        digits_pattern += \"]+\"\n",
        "\n",
        "        complete_digit_pattern = (\n",
        "            r\"^\"\n",
        "            + digits_pattern\n",
        "            + \"(?=\\s)|(?<=\\s)\"\n",
        "            + digits_pattern\n",
        "            + \"(?=\\s)|(?<=\\s)\"\n",
        "            + digits_pattern\n",
        "            + \"$\"\n",
        "        )\n",
        "\n",
        "        normalized_text = re.sub(complete_digit_pattern, \" \", normalized_text)\n",
        "\n",
        "    if config[\"rm_diacritics\"]:\n",
        "        normalized_text = unidecode(normalized_text)\n",
        "\n",
        "    normalized_text = re.sub(r\"‚Äî\", \" \", normalized_text).strip()\n",
        "    normalized_text = re.sub(r\"-\", \" \", normalized_text).strip()\n",
        "    # Remove extra spaces\n",
        "    normalized_text = re.sub(r\"\\s+\", \" \", normalized_text).strip()\n",
        "\n",
        "    return normalized_text\n",
        "\n",
        "def normalize_uroman(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"([^a-z' ])\", \" \", text)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def get_uroman_tokens(norm_transcripts, uroman_root_dir, iso = None):\n",
        "    tf = tempfile.NamedTemporaryFile()\n",
        "    tf2 = tempfile.NamedTemporaryFile()\n",
        "    with open(tf.name, \"w\") as f:\n",
        "        for t in norm_transcripts:\n",
        "            f.write(t + \"\\n\")\n",
        "\n",
        "    assert os.path.exists(f\"{uroman_root_dir}/uroman.pl\"), \"uroman not found\"\n",
        "    cmd = f\"perl {uroman_root_dir}/uroman.pl\"\n",
        "    cmd +=  f\" < {tf.name} > {tf2.name}\"\n",
        "    os.system(cmd)\n",
        "    outtexts = []\n",
        "    with open(tf2.name) as f:\n",
        "        for line in f:\n",
        "            line = \" \".join(line.strip())\n",
        "            line =  re.sub(r\"\\s+\", \" \", line).strip()\n",
        "            outtexts.append(line)\n",
        "    assert len(outtexts) == len(norm_transcripts)\n",
        "    uromans = []\n",
        "    for ot in outtexts:\n",
        "        uromans.append(normalize_uroman(ot))\n",
        "    return uromans\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2dR8cnk1ZK2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Returns timestamps of tokens and assosciated scores\n",
        "def align(emission, tokens):\n",
        "    targets = torch.tensor([tokens], dtype=torch.int32, device=device)\n",
        "    alignments, scores = F.forced_align(emission, targets, blank=0)\n",
        "\n",
        "    alignments, scores = alignments[0], scores[0]\n",
        "    scores = scores.exp()\n",
        "    return alignments, scores\n",
        "\n",
        "#Converts token-level alignment to word-level alignment\n",
        "def unflatten(list_, lengths):\n",
        "    assert len(list_) == sum(lengths)\n",
        "    i = 0\n",
        "    ret = []\n",
        "    for l in lengths:\n",
        "        ret.append(list_[i : i + l])\n",
        "        i += l\n",
        "    return ret\n",
        "\n",
        "def getAlignment(dp):\n",
        "    y, sr = dp['audio']['array'], dp['audio']['sampling_rate']\n",
        "    transcripts = dp['text'].split(\"\\n\")\n",
        "\n",
        "    norm_transcripts = [text_normalize(line.strip(), \"hi\") for line in transcripts]\n",
        "    norm_transcripts2 = []\n",
        "    for t in norm_transcripts:\n",
        "        norm_transcripts2.append(t.split())\n",
        "\n",
        "    norm_transcripts2 = [word for sent in norm_transcripts2 for word in sent]\n",
        "    token_trans = get_uroman_tokens(norm_transcripts2, \"uroman/bin/\", \"hi\")\n",
        "    norm_transcripts2 = [\"***\"] + norm_transcripts2\n",
        "    token_trans = [\"***\"] + [\"\".join(word.split()) for word in token_trans]\n",
        "    token_trans2 = [char for word in token_trans for char in word]\n",
        "\n",
        "    audio_chunk = torch.tensor([y]).to(torch.float32).to(device)\n",
        "    with torch.inference_mode():\n",
        "        emission, _ = model(audio_chunk)\n",
        "#     print(token_trans)\n",
        "#     print(token_trans2)\n",
        "    tokenized_transcript = [DICTIONARY[char.lower()] for char in token_trans2]\n",
        "    aligned_tokens, alignment_scores = align(emission, tokenized_transcript)\n",
        "    token_spans = F.merge_tokens(aligned_tokens, alignment_scores)\n",
        "    word_spans = unflatten(token_spans, [len(word) for word in token_trans])\n",
        "    output = []\n",
        "    ratio = audio_chunk.size(1) / emission.size(1)\n",
        "    for i in range(len(token_trans)):\n",
        "        sc = []\n",
        "        character_bd = []\n",
        "        for score in range(len(word_spans[i])):\n",
        "            sc.append(word_spans[i][score].score)\n",
        "            character_bd.append([word_spans[i][score].start, word_spans[i][score].end, word_spans[i][score].score])\n",
        "        sc = np.sum(sc)/len(sc)\n",
        "        output.append({\n",
        "              'word_hi':norm_transcripts2[i],\n",
        "              'word_en':token_trans[i],\n",
        "              'start':int(ratio * word_spans[i][0].start)/sr,\n",
        "              'stop':int(ratio * word_spans[i][-1].end)/sr,\n",
        "              'score':sc,\n",
        "              'character':character_bd\n",
        "          })\n",
        "    del emission\n",
        "    del audio_chunk\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    del gc\n",
        "    return output"
      ],
      "metadata": {
        "id": "Vq4hfrSni8OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The below function takes some time to run. Additionally, for some files, it runs out of GPU memory in Colab, however works fine in Kaggle.\n",
        "# I have saved the results from kaggle and downloaded it in the next cell instead of running this cell in Colab.\n",
        "\n",
        "bundle = torchaudio.pipelines.MMS_FA\n",
        "model = bundle.get_model(with_star=True).to(device)\n",
        "DICTIONARY = bundle.get_dict()\n",
        "\n",
        "out = {}\n",
        "# rewrite JSON dump function to make it more efficient\n",
        "for i in tqdm(range(len(dataset))):\n",
        "    audio = \"audio_\"+str(i+1)+\".mp3\"\n",
        "    dp = dataset[i]\n",
        "    try:\n",
        "        alignment = getAlignment(dp)\n",
        "        out[dp['audio']['path']] = alignment\n",
        "    except Exception as e:\n",
        "        print(\"Error occured with \"+audio)\n",
        "        print(e)\n",
        "        out[dp['audio']['path']] = \"Memory Error\"\n",
        "    with open(path+'speech_text_alignment.json', 'w') as file:\n",
        "        json.dump(out, file, indent=4)\n",
        "\n",
        "del model\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "del gc"
      ],
      "metadata": {
        "id": "ceecnSh5jHj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id \"1_0FrerOkUXG1tKGi9PHndToHcajS7DdH\" -O bible/speech_text_alignment.json"
      ],
      "metadata": {
        "id": "IAsKFApfaNcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"bible/speech_text_alignment.json\", \"r\") as file:\n",
        "    out = json.load(file)"
      ],
      "metadata": {
        "id": "zhLcbiplaePq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Audio-text alignment score analysis\n",
        "\n",
        "Initial few seconds of audio files is not transcribed in text files and if not accounted for could hurt STT and TTS applications alot. The star (*) token indicates the time in audio file which is not accounted in text.\n",
        "\n",
        "Additionally, we also analyse the alignment scores for words to check for errors and improvement mechanisms."
      ],
      "metadata": {
        "id": "l4noWwsYiGxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = []\n",
        "low_scores = []\n",
        "for file in tqdm(list(out.keys())):\n",
        "    for di in out[file]:\n",
        "        if (di['word_hi']==\"***\"):\n",
        "            start_time.append([file, di['stop']])\n",
        "        else:\n",
        "            if di['score']<0.2:\n",
        "              low_scores.append([file, di['word_hi'], di['word_en'], di['start'], di['stop'], di['score']])"
      ],
      "metadata": {
        "id": "UGlwqFzPi5vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = pd.DataFrame(start_time, columns=['file', 'start'])\n",
        "low_scores = pd.DataFrame(low_scores, columns=['file', 'word_hi', 'word_en', 'start', 'stop', 'score'])"
      ],
      "metadata": {
        "id": "9DCskl-YdZMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the dataframe below, it can be seen that many words in each audio have low scores. Based on playing these specific errors in audio file, we see that primarily there are two types of mistakes:\n",
        "1. Word is transcribed wrongly in the text.\n",
        "2. The uromanization is not correct.\n",
        "    - Bindu(.) = 'm' where in some cases it is 'n'. Eg. ‡§∏‡§Ç‡§§‡§æ‡§®\t= samtaan.\n",
        "    - Variant of \"sh\" in hindi - ‡§™‡§ï‡•ç‡§∑\t= pakssa.\n",
        "    - Variant of \"dh\" - ‡§™‡•Ä‡§¢‡§º‡§ø‡§Ø‡•ã‡§Ç =\tpiiddhiyom\n",
        "\n",
        "There are many such examples, which tells us that first and foremost we need a better transliteration model. The other open source models I tried were giving worse results than this, hence, a custom one needs to be built or finetuned."
      ],
      "metadata": {
        "id": "Fdv2wQyii2Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "low_scores.iloc[:20]"
      ],
      "metadata": {
        "id": "4mRQXfIkduUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the below dataframe, it can be seen that every audio file needs to be truncated to remove untranscribed audio."
      ],
      "metadata": {
        "id": "y4Ve3FD7jc-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time.head()"
      ],
      "metadata": {
        "id": "oyQDNqJNkSL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4.2: Word-Level Speech-Text Analysis"
      ],
      "metadata": {
        "id": "LnBk-A6LaY4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Word pitch similarity analysis using mean, variance (Similarity Metric: DTW - Dynamic Time Warping)\n",
        "- Word similarity analysis using mean, variance (Similarity metric: DTW)"
      ],
      "metadata": {
        "id": "qVR3dWqUjoQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path+\"speech_text_alignment.json\", \"r\") as file:\n",
        "    alignment = json.load(file)"
      ],
      "metadata": {
        "id": "Vz9xXdeAaZT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_mapper = []\n",
        "for key in list(alignment.keys()):\n",
        "    for item in alignment[key]:\n",
        "        item['file'] = key\n",
        "        word_mapper.append(item)\n",
        "word_mapper = pd.DataFrame(word_mapper)\n",
        "word_mapper_clean = word_mapper[word_mapper['word_hi']!=\"***\"]\n",
        "unique_words = word_mapper_clean['word_hi'].unique()\n",
        "\n",
        "stop_words = ['‡§Æ‡•à‡§Ç', '‡§Æ‡•Å‡§ù‡§ï‡•ã', '‡§Æ‡•á‡§∞‡§æ', '‡§Ö‡§™‡§®‡•á ‡§Ü‡§™ ‡§ï‡•ã', '‡§π‡§Æ‡§®‡•á', '‡§π‡§Æ‡§æ‡§∞‡§æ', '‡§Ö‡§™‡§®‡§æ', '‡§π‡§Æ', '‡§Ü‡§™', '‡§Ü‡§™‡§ï‡§æ', '‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§∞‡§æ', '‡§Ö‡§™‡§®‡•á ‡§Ü‡§™', '‡§∏‡•ç‡§µ‡§Ø‡§Ç', '‡§µ‡§π', '‡§á‡§∏‡•á', '‡§â‡§∏‡§ï‡•á', '‡§ñ‡•Å‡§¶ ‡§ï‡•ã', '‡§ï‡§ø ‡§µ‡§π', '‡§â‡§∏‡§ï‡•Ä', '‡§â‡§∏‡§ï‡§æ', '‡§ñ‡•Å‡§¶ ‡§π‡•Ä', '‡§Ø‡§π', '‡§á‡§∏‡§ï‡•á', '‡§â‡§®‡•ç‡§π‡•ã‡§®‡•á', '‡§Ö‡§™‡§®‡•á', '‡§ï‡•ç‡§Ø‡§æ', '‡§ú‡•ã', '‡§ï‡§ø‡§∏‡•á', '‡§ï‡§ø‡§∏‡§ï‡•ã', '‡§ï‡§ø', '‡§Ø‡•á', '‡§π‡•Ç‡§Å', '‡§π‡•ã‡§§‡§æ ‡§π‡•à', '‡§∞‡§π‡•á', '‡§•‡•Ä', '‡§•‡•á', '‡§π‡•ã‡§®‡§æ', '‡§ó‡§Ø‡§æ', '‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à', '‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à', '‡§π‡•à', '‡§™‡§°‡§æ', '‡§π‡•ã‡§®‡•á', '‡§ï‡§∞‡§®‡§æ', '‡§ï‡§∞‡§§‡§æ ‡§π‡•à', '‡§ï‡§ø‡§Ø‡§æ', '‡§∞‡§π‡•Ä', '‡§è‡§ï', '‡§≤‡•á‡§ï‡§ø‡§®', '‡§Ö‡§ó‡§∞', '‡§Ø‡§æ', '‡§ï‡•ç‡§Ø‡•Ç‡§Ç‡§ï‡§ø', '‡§ú‡•à‡§∏‡§æ', '‡§ú‡§¨ ‡§§‡§ï', '‡§ú‡§¨‡§ï‡§ø', '‡§ï‡•Ä', '‡§™‡§∞', '‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ', '‡§ï‡•á ‡§≤‡§ø‡§è', '‡§∏‡§æ‡§•', '‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç', '‡§ñ‡§ø‡§≤‡§æ‡§´', '‡§¨‡•Ä‡§ö', '‡§Æ‡•á‡§Ç', '‡§ï‡•á ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ ‡§∏‡•á', '‡§¶‡•å‡§∞‡§æ‡§®', '‡§∏‡•á ‡§™‡§π‡§≤‡•á', '‡§ï‡•á ‡§¨‡§æ‡§¶', '‡§ä‡§™‡§∞', '‡§®‡•Ä‡§ö‡•á', '‡§ï‡•ã', '‡§∏‡•á', '‡§§‡§ï', '‡§∏‡•á ‡§®‡•Ä‡§ö‡•á', '‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç', '‡§®‡§ø‡§ï‡§≤', '‡§¨‡§Ç‡§¶', '‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï', '‡§§‡§π‡§§', '‡§¶‡•Å‡§¨‡§æ‡§∞‡§æ', '‡§Ü‡§ó‡•á', '‡§´‡§ø‡§∞', '‡§è‡§ï ‡§¨‡§æ‡§∞', '‡§Ø‡§π‡§æ‡§Å', '‡§µ‡§π‡§æ‡§Å', '‡§ï‡§¨', '‡§ï‡§π‡§æ‡§Å', '‡§ï‡•ç‡§Ø‡•ã‡§Ç', '‡§ï‡•à‡§∏‡•á', '‡§∏‡§æ‡§∞‡•á', '‡§ï‡§ø‡§∏‡•Ä', '‡§¶‡•ã‡§®‡•ã', '‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï', '‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ', '‡§Ö‡§ß‡§ø‡§ï‡§æ‡§Ç‡§∂', '‡§Ö‡§®‡•ç‡§Ø', '‡§Æ‡•á‡§Ç ‡§ï‡•Å‡§õ', '‡§ê‡§∏‡§æ', '‡§Æ‡•á‡§Ç ‡§ï‡•ã‡§à', '‡§Æ‡§æ‡§§‡•ç‡§∞', '‡§ñ‡•Å‡§¶', '‡§∏‡§Æ‡§æ‡§®', '‡§á‡§∏‡§≤‡§ø‡§è', '‡§¨‡§π‡•Å‡§§', '‡§∏‡§ï‡§§‡§æ', '‡§ú‡§æ‡§Ø‡•á‡§Ç‡§ó‡•á', '‡§ú‡§∞‡§æ', '‡§ö‡§æ‡§π‡§ø‡§è', '‡§Ö‡§≠‡•Ä', '‡§î‡§∞', '‡§ï‡§∞ ‡§¶‡§ø‡§Ø‡§æ', '‡§∞‡§ñ‡•á‡§Ç', '‡§ï‡§æ', '‡§π‡•à‡§Ç', '‡§á‡§∏', '‡§π‡•ã‡§§‡§æ', '‡§ï‡§∞‡§®‡•á', '‡§®‡•á', '‡§¨‡§®‡•Ä', '‡§§‡•ã', '‡§π‡•Ä', '‡§π‡•ã', '‡§á‡§∏‡§ï‡§æ', '‡§•‡§æ', '‡§π‡•Å‡§Ü', '‡§µ‡§æ‡§≤‡•á', '‡§¨‡§æ‡§¶', '‡§≤‡§ø‡§è', '‡§∏‡§ï‡§§‡•á', '‡§á‡§∏‡§Æ‡•á‡§Ç', '‡§¶‡•ã', '‡§µ‡•á', '‡§ï‡§∞‡§§‡•á', '‡§ï‡§π‡§æ', '‡§µ‡§∞‡•ç‡§ó', '‡§ï‡§à', '‡§ï‡§∞‡•á‡§Ç', '‡§π‡•ã‡§§‡•Ä', '‡§Ö‡§™‡§®‡•Ä', '‡§â‡§®‡§ï‡•á', '‡§Ø‡§¶‡§ø', '‡§π‡•Å‡§à', '‡§ú‡§æ', '‡§ï‡§π‡§§‡•á', '‡§ú‡§¨', '‡§π‡•ã‡§§‡•á', '‡§ï‡•ã‡§à', '‡§π‡•Å‡§è', '‡§µ', '‡§ú‡•à‡§∏‡•á', '‡§∏‡§≠‡•Ä', '‡§ï‡§∞‡§§‡§æ', '‡§â‡§®‡§ï‡•Ä', '‡§§‡§∞‡§π', '‡§â‡§∏', '‡§Ü‡§¶‡§ø', '‡§á‡§∏‡§ï‡•Ä', '‡§â‡§®‡§ï‡§æ', '‡§á‡§∏‡•Ä', '‡§™‡•á', '‡§§‡§•‡§æ', '‡§≠‡•Ä', '‡§™‡§∞‡§Ç‡§§‡•Å', '‡§á‡§®', '‡§ï‡§Æ', '‡§¶‡•Ç‡§∞', '‡§™‡•Ç‡§∞‡•á', '‡§ó‡§Ø‡•á', '‡§§‡•Å‡§Æ', '‡§Æ‡•à', '‡§Ø‡§π‡§æ‡§Ç', '‡§π‡•Å‡§Ø‡•á', '‡§ï‡§≠‡•Ä', '‡§Ö‡§•‡§µ‡§æ', '‡§ó‡§Ø‡•Ä', '‡§™‡•ç‡§∞‡§§‡§ø', '‡§ú‡§æ‡§§‡§æ', '‡§á‡§®‡•ç‡§π‡•á‡§Ç', '‡§ó‡§à', '‡§Ö‡§¨', '‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç', '‡§≤‡§ø‡§Ø‡§æ', '‡§¨‡§°‡§º‡§æ', '‡§ú‡§æ‡§§‡•Ä', '‡§§‡§¨', '‡§â‡§∏‡•á', '‡§ú‡§æ‡§§‡•á', '‡§≤‡•á‡§ï‡§∞', '‡§¨‡§°‡§º‡•á', '‡§¶‡•Ç‡§∏‡§∞‡•á', '‡§ú‡§æ‡§®‡•á', '‡§¨‡§æ‡§π‡§∞', '‡§∏‡•ç‡§•‡§æ‡§®', '‡§â‡§®‡•ç‡§π‡•á‡§Ç ', '‡§ó‡§è', '‡§ê‡§∏‡•á', '‡§ú‡§ø‡§∏‡§∏‡•á', '‡§∏‡§Æ‡§Ø', '‡§¶‡•ã‡§®‡•ã‡§Ç', '‡§ï‡§ø‡§è', '‡§∞‡§π‡§§‡•Ä', '‡§á‡§®‡§ï‡•á', '‡§á‡§®‡§ï‡§æ', '‡§á‡§®‡§ï‡•Ä', '‡§∏‡§ï‡§§‡•Ä', '‡§Ü‡§ú', '‡§ï‡§≤', '‡§ú‡§ø‡§®‡•ç‡§π‡•á‡§Ç', '‡§ú‡§ø‡§®‡•ç‡§π‡•ã‡§Ç', '‡§§‡§ø‡§®‡•ç‡§π‡•á‡§Ç', '‡§§‡§ø‡§®‡•ç‡§π‡•ã‡§Ç', '‡§ï‡§ø‡§®‡•ç‡§π‡•ã‡§Ç', '‡§ï‡§ø‡§®‡•ç‡§π‡•á‡§Ç', '‡§á‡§§‡•ç‡§Ø‡§æ‡§¶‡§ø', '‡§á‡§®‡•ç‡§π‡•ã‡§Ç', '‡§â‡§®‡•ç‡§π‡•ã‡§Ç', '‡§¨‡§ø‡§≤‡§ï‡•Å‡§≤', '‡§®‡§ø‡§π‡§æ‡§Ø‡§§', '‡§á‡§®‡•ç‡§π‡•Ä‡§Ç', '‡§â‡§®‡•ç‡§π‡•Ä‡§Ç', '‡§ú‡§ø‡§§‡§®‡§æ', '‡§¶‡•Ç‡§∏‡§∞‡§æ', '‡§ï‡§ø‡§§‡§®‡§æ', '‡§∏‡§æ‡§¨‡•Å‡§§', '‡§µ‡•ö‡•à‡§∞‡§π', '‡§ï‡•å‡§®‡§∏‡§æ', '‡§≤‡§ø‡§Ø‡•á', '‡§¶‡§ø‡§Ø‡§æ', '‡§ú‡§ø‡§∏‡•á', '‡§§‡§ø‡§∏‡•á', '‡§ï‡§æ‡•û‡•Ä', '‡§™‡§π‡§≤‡•á', '‡§¨‡§æ‡§≤‡§æ', '‡§Æ‡§æ‡§®‡•ã', '‡§Ö‡§Ç‡§¶‡§∞', '‡§≠‡•Ä‡§§‡§∞', '‡§™‡•Ç‡§∞‡§æ', '‡§∏‡§æ‡§∞‡§æ', '‡§â‡§®‡§ï‡•ã', '‡§µ‡§π‡•Ä‡§Ç', '‡§ú‡§π‡§æ‡§Å', '‡§ú‡•Ä‡§ß‡§∞', '‡§ï‡•á', '‡§è‡§µ‡§Ç', '‡§ï‡•Å‡§õ', '‡§ï‡•Å‡§≤', '‡§∞‡§π‡§æ', '‡§ú‡§ø‡§∏', '‡§ú‡§ø‡§®', '‡§§‡§ø‡§∏', '‡§§‡§ø‡§®', '‡§ï‡•å‡§®', '‡§ï‡§ø‡§∏', '‡§∏‡§Ç‡§ó', '‡§Ø‡§π‡•Ä', '‡§¨‡§π‡•Ä', '‡§â‡§∏‡•Ä', '‡§Æ‡§ó‡§∞', '‡§ï‡§∞', '‡§Æ‡•á', '‡§è‡§∏', '‡§â‡§®', '‡§∏‡•ã', '‡§Ö‡§§', '‡§π‡•Ç‡§Ç', '‡§®']\n",
        "paths = []\n",
        "for i in tqdm(range(len(dataset))):\n",
        "    paths.append(dataset[i]['audio']['path'])\n",
        "\n",
        "mapper = {}\n",
        "for i in range(len(paths)):\n",
        "    mapper[paths[i]] = i\n",
        "\n",
        "word_mapper2 = word_mapper_clean[~word_mapper_clean['word_hi'].isin(stop_words)]\n",
        "word_mapper2.index = np.arange(len(word_mapper2))\n",
        "\n",
        "sample_rate = 16000\n",
        "# Doing similarity analysis just for 20 occurances of 20 words with highest count due to high computation costs\n",
        "most_freq = list(dict(word_mapper2['word_hi'].value_counts()).keys())[:20]\n",
        "n = 20\n",
        "word_pitch = {}\n",
        "word_audio = {}\n",
        "word_duration = {}\n",
        "for word in tqdm(most_freq):\n",
        "    topn = word_mapper2[word_mapper2['word_hi']==word].drop_duplicates(subset='file', keep='first')[:n]\n",
        "    topn = topn[['file', 'start','stop']]\n",
        "    topn['start'] = topn['start'] * sample_rate\n",
        "    topn['stop'] = topn['stop'] * sample_rate\n",
        "    pitch = []\n",
        "    audio_seq = []\n",
        "    audio_duration = []\n",
        "    for i, row in topn.iterrows():\n",
        "        audio = dataset.select([mapper[row['file']]])['audio'][0]['array'][int(row['start']): int(row['stop'])]\n",
        "        pitch.append(torchaudio.functional.detect_pitch_frequency(waveform=torch.tensor(audio), sample_rate=sample_rate))\n",
        "        audio_seq.append(audio)\n",
        "        audio_duration.append(len(audio))\n",
        "    audio_duration = pd.Series(audio_duration)\n",
        "    word_duration[word] = [audio_duration.mean(), audio_duration.std()]\n",
        "    sim_score = []\n",
        "    for i in range(len(pitch)):\n",
        "        for j in range(i, len(pitch)):\n",
        "            if i!=j:\n",
        "                sim = fastdtw(pitch[i], pitch[j])[0]\n",
        "                sim_score.append([i,j,sim])\n",
        "    sim_score = pd.DataFrame(sim_score, columns=['audio1','audio2','sim'])\n",
        "    sim_score = sim_score.sort_values('sim', ascending=False)\n",
        "    pmean = sim_score['sim'].mean()\n",
        "    pstd = sim_score['sim'].std()\n",
        "    sim_score = []\n",
        "    for i in range(len(audio_seq)):\n",
        "        for j in range(i, len(audio_seq)):\n",
        "            if i!=j:\n",
        "                sim = fastdtw(audio_seq[i], audio_seq[j])[0]\n",
        "                sim_score.append([i,j,sim])\n",
        "    sim_score = pd.DataFrame(sim_score, columns=['audio1','audio2','sim'])\n",
        "    sim_score = sim_score.sort_values('sim', ascending=False)\n",
        "    amean = sim_score['sim'].mean()\n",
        "    astd = sim_score['sim'].std()\n",
        "    word_pitch[word] = [pmean, pstd]\n",
        "    word_audio[word] = [amean, astd]"
      ],
      "metadata": {
        "id": "lqXqLTS3kCy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lower DTW -> Higher Similarity\n",
        "# high DTW -> Low similarity"
      ],
      "metadata": {
        "id": "TsWlpmxUk4Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pitch_ = []\n",
        "for key, val in word_pitch.items():\n",
        "    temp = [key]\n",
        "    temp.extend(val)\n",
        "    pitch_.append(temp)\n",
        "pitch_ = pd.DataFrame(pitch_, columns=['word', 'mean_pitch_sim', 'std_pitch_sim'])\n",
        "pitch_ = pitch_.sort_values('mean_pitch_sim')\n",
        "pitch_.head()"
      ],
      "metadata": {
        "id": "UJiRZbZWfOEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DTW takes into account different speaking speeds\n",
        "# Lower similarity can indicate variations in pronounciations or pitch"
      ],
      "metadata": {
        "id": "b0xRN2WFlico"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_feats = []\n",
        "for key, val in word_audio.items():\n",
        "    temp = [key]\n",
        "    temp.extend(val)\n",
        "    audio_feats.append(temp)\n",
        "audio_feats = pd.DataFrame(audio_feats, columns=['word', 'mean_audio_sim', 'std_audio_sim'])\n",
        "audio_feats = audio_feats.sort_values('mean_audio_sim')\n",
        "audio_feats.head()"
      ],
      "metadata": {
        "id": "pNsBn03_lfax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Words with high similarity and low standard deviation should have more variations in the dataset. To improve ASR perfomance, different variations with low similarity should be added to dataset."
      ],
      "metadata": {
        "id": "VxTz9uT2FCBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "for idx, tensor_data in enumerate(pitch):\n",
        "    plt.plot(tensor_data, label=f'Chunk {idx + 1}')\n",
        "\n",
        "plt.title('Pitch Measurements for the word \"Nagar\"')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Pitch Value (Hz)')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "rjSY7D4okHF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_score"
      ],
      "metadata": {
        "id": "Cinai-7_nq-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Audio(audio_seq[10], rate=sample_rate)"
      ],
      "metadata": {
        "id": "BdqKtSbpkHCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IPython.display.Audio(audio_seq[19], rate=sample_rate)"
      ],
      "metadata": {
        "id": "SDgnSCWBkHAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4.3: Audio-based Anomaly Detection"
      ],
      "metadata": {
        "id": "nvw_eoaOZLkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading PCA result on FFT since it runs out of RAM in Colab. The next cell was run in Kaggle and results were saved and loaded here.\n",
        "!gdown --id \"1s0vG6Lp9jTMPCzrquz193xSwy7qf_3Ja\" -O bible/fft_all.txt"
      ],
      "metadata": {
        "id": "Qh7M3By5ojnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_fft(y, fs):\n",
        "    T = 1.0 / fs\n",
        "    N = y.shape[0]\n",
        "    yf = fft(y)\n",
        "    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
        "    vals = 2.0/N * np.abs(yf[0:N//2])\n",
        "    return xf, vals\n",
        "\n",
        "fft_all = []\n",
        "names = []\n",
        "sr = 16000\n",
        "full_audio_duration = []\n",
        "for i in tqdm(range(len(dataset))):\n",
        "    dp = dataset[i]\n",
        "    samples = dp['audio']['array']\n",
        "    full_audio_duration.append(len(samples)/sr)\n",
        "    x, val = custom_fft(samples, sr)\n",
        "    fft_all.append(val)\n",
        "    names.append(dp['audio']['path'])\n",
        "\n",
        "mini = np.inf\n",
        "for fft_ in fft_all:\n",
        "    mini = min(mini, len(fft_))\n",
        "fft_all2 = []\n",
        "for fft_ in fft_all:\n",
        "    fft_all2.append(fft_[:mini])\n",
        "\n",
        "fft_all2 = np.array(fft_all2)\n",
        "fft_all2 = (fft_all2 - np.mean(fft_all2, axis=0)) / np.std(fft_all2, axis=0)\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "fft_all2 = pca.fit_transform(fft_all2)"
      ],
      "metadata": {
        "id": "Z3Vxl3sJZMmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fft_all2 = []\n",
        "with open(path+'fft_all.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        array = np.fromstring(line.strip()[1:-1], sep=' ')\n",
        "        fft_all2.append(array)"
      ],
      "metadata": {
        "id": "SCIOEOlapYbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fft_all2[0]"
      ],
      "metadata": {
        "id": "UEcFZ-3qqHOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "import plotly.offline as pyo\n",
        "pyo.init_notebook_mode(connected=True)"
      ],
      "metadata": {
        "id": "KtpGzeQzqfh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_3d_plot(data, names):\n",
        "    data = np.array(data)\n",
        "    scatt = go.Scatter3d(x=data[:, 0], y=data[:, 1], z=data[:, 2], mode='markers', text=names)\n",
        "    plot_data = [scatt]\n",
        "    layout = go.Layout(title=\"Anomaly detection\")\n",
        "    figure = go.Figure(data=plot_data, layout=layout)\n",
        "    # pyo.iplot(figure)\n",
        "    figure.show(renderer=\"colab\")\n",
        "\n",
        "interactive_3d_plot(fft_all2, paths);"
      ],
      "metadata": {
        "id": "by0Ul4VHhp5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After playing the audio anomalies as indicated by the plot, I couldn't find any significant difference in the audio files as of now. This would need further  investigation."
      ],
      "metadata": {
        "id": "9blmn6L-FcwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4.4: Audio Duration Analysis"
      ],
      "metadata": {
        "id": "XKVzUWRJZNAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word level - Only for top 20 most frequent non-stop words"
      ],
      "metadata": {
        "id": "oYBhiXocw0ZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(word_duration.keys())\n",
        "mean_durations = [word_duration[word][0] for word in words]\n",
        "std_devs = [word_duration[word][1] for word in words]\n",
        "\n",
        "# Create the plot\n",
        "plt.rcParams.update(plt.rcParamsDefault)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(get_uroman_tokens(words,\"uroman/bin/\", \"hi\"), mean_durations, yerr=std_devs, capsize=5)\n",
        "plt.xlabel('Hindi Words')\n",
        "plt.ylabel('Mean Duration (ms)')\n",
        "plt.title('Mean Duration of Hindi Word Speech Snippets with Standard Deviation')\n",
        "plt.xticks(rotation=90)  # Rotate the x-axis labels for better readability\n",
        "plt.tight_layout()  # Adjust the layout for better fit\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GMLOPYmVoRl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Audio level"
      ],
      "metadata": {
        "id": "SYU6tcV4xK4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "for file in alignment.keys():\n",
        "    for word in alignment[file]:\n",
        "        if word['word_hi']!='***':\n",
        "            word['file'] = file\n",
        "            words.append(word)\n",
        "words = pd.DataFrame(words)\n",
        "words['duration'] = words['stop']-words['start']\n",
        "words = words[['file', 'word_hi', 'word_en', 'duration']]\n",
        "grouped_duration = words.groupby('file').sum('duration')\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(grouped_duration['duration'].values)\n",
        "plt.title('Boxplot of Durations by File')\n",
        "plt.xlabel('File')\n",
        "plt.ylabel('File Duration')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "B6_O-Q40w7Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = words.groupby('file').agg({'duration': 'sum', 'word_en': 'count'})\n",
        "grouped['total_chars'] = words.groupby('file')['word_en'].apply(lambda x: x.str.len().sum())\n",
        "\n",
        "# Calculate word speed (words per second) and character speed (characters per second)\n",
        "grouped['word_speed'] = grouped['word_en'] / grouped['duration']\n",
        "grouped['char_speed'] = grouped['total_chars'] / grouped['duration']\n",
        "print(grouped[['word_speed', 'char_speed']])"
      ],
      "metadata": {
        "id": "HT2Xltxw1ON_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(y=grouped['word_speed'])\n",
        "plt.title('Boxplot of Word Speed by File')\n",
        "plt.xlabel('Word Speed (words per second)')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "qveUUO0i22r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(y=grouped['char_speed'])\n",
        "plt.title('Boxplot of Character Speed by File')\n",
        "plt.xlabel('Character Speed (characters per second)')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "A_CWSK6sZNep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(y=words.groupby('word_hi').agg({'duration':'mean'}).sort_values('duration', ascending=False)['duration'].values)\n",
        "plt.title('Boxplot of Mean Word Duration')\n",
        "plt.xlabel('Mean Word Duration (seconds)')\n",
        "plt.ylabel('Duration (seconds)')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "mGsdUDu837--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array = words.groupby('word_hi').agg({'duration':'std'}).sort_values('duration', ascending=False)['duration'].values\n",
        "filtered_array = array[~np.isnan(array)]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(y=filtered_array)\n",
        "plt.title('Boxplot of standard deviations of word durations')\n",
        "plt.xlabel('Std Word Duration (seconds)')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "xiJppdGD376p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4.5: Audio Quality Analysis"
      ],
      "metadata": {
        "id": "wZ9fZ3d1ZNzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def signaltonoise(a, axis=0, ddof=0):\n",
        "    a = np.asanyarray(a)\n",
        "    m = a.mean(axis)\n",
        "    sd = a.std(axis=axis, ddof=ddof)\n",
        "    return np.where(sd == 0, 0, m/sd)\n",
        "\n",
        "noise = {}\n",
        "for i in tqdm(range(len(dataset))):\n",
        "    noise[dataset['audio'][i]['path']] = signaltonoise(dataset['audio'][i]['array'])"
      ],
      "metadata": {
        "id": "OGYdNBG6ZOUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4.6: Word-level Text Analysis"
      ],
      "metadata": {
        "id": "d_iO47xsZPOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = dataset['text']\n",
        "sentences = []\n",
        "nsen = []\n",
        "stop_words = ['‡§Æ‡•à‡§Ç', '‡§Æ‡•Å‡§ù‡§ï‡•ã', '‡§Æ‡•á‡§∞‡§æ', '‡§Ö‡§™‡§®‡•á ‡§Ü‡§™ ‡§ï‡•ã', '‡§π‡§Æ‡§®‡•á', '‡§π‡§Æ‡§æ‡§∞‡§æ', '‡§Ö‡§™‡§®‡§æ', '‡§π‡§Æ', '‡§Ü‡§™', '‡§Ü‡§™‡§ï‡§æ', '‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§∞‡§æ', '‡§Ö‡§™‡§®‡•á ‡§Ü‡§™', '‡§∏‡•ç‡§µ‡§Ø‡§Ç', '‡§µ‡§π', '‡§á‡§∏‡•á', '‡§â‡§∏‡§ï‡•á', '‡§ñ‡•Å‡§¶ ‡§ï‡•ã', '‡§ï‡§ø ‡§µ‡§π', '‡§â‡§∏‡§ï‡•Ä', '‡§â‡§∏‡§ï‡§æ', '‡§ñ‡•Å‡§¶ ‡§π‡•Ä', '‡§Ø‡§π', '‡§á‡§∏‡§ï‡•á', '‡§â‡§®‡•ç‡§π‡•ã‡§®‡•á', '‡§Ö‡§™‡§®‡•á', '‡§ï‡•ç‡§Ø‡§æ', '‡§ú‡•ã', '‡§ï‡§ø‡§∏‡•á', '‡§ï‡§ø‡§∏‡§ï‡•ã', '‡§ï‡§ø', '‡§Ø‡•á', '‡§π‡•Ç‡§Å', '‡§π‡•ã‡§§‡§æ ‡§π‡•à', '‡§∞‡§π‡•á', '‡§•‡•Ä', '‡§•‡•á', '‡§π‡•ã‡§®‡§æ', '‡§ó‡§Ø‡§æ', '‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à', '‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à', '‡§π‡•à', '‡§™‡§°‡§æ', '‡§π‡•ã‡§®‡•á', '‡§ï‡§∞‡§®‡§æ', '‡§ï‡§∞‡§§‡§æ ‡§π‡•à', '‡§ï‡§ø‡§Ø‡§æ', '‡§∞‡§π‡•Ä', '‡§è‡§ï', '‡§≤‡•á‡§ï‡§ø‡§®', '‡§Ö‡§ó‡§∞', '‡§Ø‡§æ', '‡§ï‡•ç‡§Ø‡•Ç‡§Ç‡§ï‡§ø', '‡§ú‡•à‡§∏‡§æ', '‡§ú‡§¨ ‡§§‡§ï', '‡§ú‡§¨‡§ï‡§ø', '‡§ï‡•Ä', '‡§™‡§∞', '‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ', '‡§ï‡•á ‡§≤‡§ø‡§è', '‡§∏‡§æ‡§•', '‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç', '‡§ñ‡§ø‡§≤‡§æ‡§´', '‡§¨‡•Ä‡§ö', '‡§Æ‡•á‡§Ç', '‡§ï‡•á ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ ‡§∏‡•á', '‡§¶‡•å‡§∞‡§æ‡§®', '‡§∏‡•á ‡§™‡§π‡§≤‡•á', '‡§ï‡•á ‡§¨‡§æ‡§¶', '‡§ä‡§™‡§∞', '‡§®‡•Ä‡§ö‡•á', '‡§ï‡•ã', '‡§∏‡•á', '‡§§‡§ï', '‡§∏‡•á ‡§®‡•Ä‡§ö‡•á', '‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç', '‡§®‡§ø‡§ï‡§≤', '‡§¨‡§Ç‡§¶', '‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï', '‡§§‡§π‡§§', '‡§¶‡•Å‡§¨‡§æ‡§∞‡§æ', '‡§Ü‡§ó‡•á', '‡§´‡§ø‡§∞', '‡§è‡§ï ‡§¨‡§æ‡§∞', '‡§Ø‡§π‡§æ‡§Å', '‡§µ‡§π‡§æ‡§Å', '‡§ï‡§¨', '‡§ï‡§π‡§æ‡§Å', '‡§ï‡•ç‡§Ø‡•ã‡§Ç', '‡§ï‡•à‡§∏‡•á', '‡§∏‡§æ‡§∞‡•á', '‡§ï‡§ø‡§∏‡•Ä', '‡§¶‡•ã‡§®‡•ã', '‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï', '‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ', '‡§Ö‡§ß‡§ø‡§ï‡§æ‡§Ç‡§∂', '‡§Ö‡§®‡•ç‡§Ø', '‡§Æ‡•á‡§Ç ‡§ï‡•Å‡§õ', '‡§ê‡§∏‡§æ', '‡§Æ‡•á‡§Ç ‡§ï‡•ã‡§à', '‡§Æ‡§æ‡§§‡•ç‡§∞', '‡§ñ‡•Å‡§¶', '‡§∏‡§Æ‡§æ‡§®', '‡§á‡§∏‡§≤‡§ø‡§è', '‡§¨‡§π‡•Å‡§§', '‡§∏‡§ï‡§§‡§æ', '‡§ú‡§æ‡§Ø‡•á‡§Ç‡§ó‡•á', '‡§ú‡§∞‡§æ', '‡§ö‡§æ‡§π‡§ø‡§è', '‡§Ö‡§≠‡•Ä', '‡§î‡§∞', '‡§ï‡§∞ ‡§¶‡§ø‡§Ø‡§æ', '‡§∞‡§ñ‡•á‡§Ç', '‡§ï‡§æ', '‡§π‡•à‡§Ç', '‡§á‡§∏', '‡§π‡•ã‡§§‡§æ', '‡§ï‡§∞‡§®‡•á', '‡§®‡•á', '‡§¨‡§®‡•Ä', '‡§§‡•ã', '‡§π‡•Ä', '‡§π‡•ã', '‡§á‡§∏‡§ï‡§æ', '‡§•‡§æ', '‡§π‡•Å‡§Ü', '‡§µ‡§æ‡§≤‡•á', '‡§¨‡§æ‡§¶', '‡§≤‡§ø‡§è', '‡§∏‡§ï‡§§‡•á', '‡§á‡§∏‡§Æ‡•á‡§Ç', '‡§¶‡•ã', '‡§µ‡•á', '‡§ï‡§∞‡§§‡•á', '‡§ï‡§π‡§æ', '‡§µ‡§∞‡•ç‡§ó', '‡§ï‡§à', '‡§ï‡§∞‡•á‡§Ç', '‡§π‡•ã‡§§‡•Ä', '‡§Ö‡§™‡§®‡•Ä', '‡§â‡§®‡§ï‡•á', '‡§Ø‡§¶‡§ø', '‡§π‡•Å‡§à', '‡§ú‡§æ', '‡§ï‡§π‡§§‡•á', '‡§ú‡§¨', '‡§π‡•ã‡§§‡•á', '‡§ï‡•ã‡§à', '‡§π‡•Å‡§è', '‡§µ', '‡§ú‡•à‡§∏‡•á', '‡§∏‡§≠‡•Ä', '‡§ï‡§∞‡§§‡§æ', '‡§â‡§®‡§ï‡•Ä', '‡§§‡§∞‡§π', '‡§â‡§∏', '‡§Ü‡§¶‡§ø', '‡§á‡§∏‡§ï‡•Ä', '‡§â‡§®‡§ï‡§æ', '‡§á‡§∏‡•Ä', '‡§™‡•á', '‡§§‡§•‡§æ', '‡§≠‡•Ä', '‡§™‡§∞‡§Ç‡§§‡•Å', '‡§á‡§®', '‡§ï‡§Æ', '‡§¶‡•Ç‡§∞', '‡§™‡•Ç‡§∞‡•á', '‡§ó‡§Ø‡•á', '‡§§‡•Å‡§Æ', '‡§Æ‡•à', '‡§Ø‡§π‡§æ‡§Ç', '‡§π‡•Å‡§Ø‡•á', '‡§ï‡§≠‡•Ä', '‡§Ö‡§•‡§µ‡§æ', '‡§ó‡§Ø‡•Ä', '‡§™‡•ç‡§∞‡§§‡§ø', '‡§ú‡§æ‡§§‡§æ', '‡§á‡§®‡•ç‡§π‡•á‡§Ç', '‡§ó‡§à', '‡§Ö‡§¨', '‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç', '‡§≤‡§ø‡§Ø‡§æ', '‡§¨‡§°‡§º‡§æ', '‡§ú‡§æ‡§§‡•Ä', '‡§§‡§¨', '‡§â‡§∏‡•á', '‡§ú‡§æ‡§§‡•á', '‡§≤‡•á‡§ï‡§∞', '‡§¨‡§°‡§º‡•á', '‡§¶‡•Ç‡§∏‡§∞‡•á', '‡§ú‡§æ‡§®‡•á', '‡§¨‡§æ‡§π‡§∞', '‡§∏‡•ç‡§•‡§æ‡§®', '‡§â‡§®‡•ç‡§π‡•á‡§Ç ', '‡§ó‡§è', '‡§ê‡§∏‡•á', '‡§ú‡§ø‡§∏‡§∏‡•á', '‡§∏‡§Æ‡§Ø', '‡§¶‡•ã‡§®‡•ã‡§Ç', '‡§ï‡§ø‡§è', '‡§∞‡§π‡§§‡•Ä', '‡§á‡§®‡§ï‡•á', '‡§á‡§®‡§ï‡§æ', '‡§á‡§®‡§ï‡•Ä', '‡§∏‡§ï‡§§‡•Ä', '‡§Ü‡§ú', '‡§ï‡§≤', '‡§ú‡§ø‡§®‡•ç‡§π‡•á‡§Ç', '‡§ú‡§ø‡§®‡•ç‡§π‡•ã‡§Ç', '‡§§‡§ø‡§®‡•ç‡§π‡•á‡§Ç', '‡§§‡§ø‡§®‡•ç‡§π‡•ã‡§Ç', '‡§ï‡§ø‡§®‡•ç‡§π‡•ã‡§Ç', '‡§ï‡§ø‡§®‡•ç‡§π‡•á‡§Ç', '‡§á‡§§‡•ç‡§Ø‡§æ‡§¶‡§ø', '‡§á‡§®‡•ç‡§π‡•ã‡§Ç', '‡§â‡§®‡•ç‡§π‡•ã‡§Ç', '‡§¨‡§ø‡§≤‡§ï‡•Å‡§≤', '‡§®‡§ø‡§π‡§æ‡§Ø‡§§', '‡§á‡§®‡•ç‡§π‡•Ä‡§Ç', '‡§â‡§®‡•ç‡§π‡•Ä‡§Ç', '‡§ú‡§ø‡§§‡§®‡§æ', '‡§¶‡•Ç‡§∏‡§∞‡§æ', '‡§ï‡§ø‡§§‡§®‡§æ', '‡§∏‡§æ‡§¨‡•Å‡§§', '‡§µ‡•ö‡•à‡§∞‡§π', '‡§ï‡•å‡§®‡§∏‡§æ', '‡§≤‡§ø‡§Ø‡•á', '‡§¶‡§ø‡§Ø‡§æ', '‡§ú‡§ø‡§∏‡•á', '‡§§‡§ø‡§∏‡•á', '‡§ï‡§æ‡•û‡•Ä', '‡§™‡§π‡§≤‡•á', '‡§¨‡§æ‡§≤‡§æ', '‡§Æ‡§æ‡§®‡•ã', '‡§Ö‡§Ç‡§¶‡§∞', '‡§≠‡•Ä‡§§‡§∞', '‡§™‡•Ç‡§∞‡§æ', '‡§∏‡§æ‡§∞‡§æ', '‡§â‡§®‡§ï‡•ã', '‡§µ‡§π‡•Ä‡§Ç', '‡§ú‡§π‡§æ‡§Å', '‡§ú‡•Ä‡§ß‡§∞', '‡§ï‡•á', '‡§è‡§µ‡§Ç', '‡§ï‡•Å‡§õ', '‡§ï‡•Å‡§≤', '‡§∞‡§π‡§æ', '‡§ú‡§ø‡§∏', '‡§ú‡§ø‡§®', '‡§§‡§ø‡§∏', '‡§§‡§ø‡§®', '‡§ï‡•å‡§®', '‡§ï‡§ø‡§∏', '‡§∏‡§Ç‡§ó', '‡§Ø‡§π‡•Ä', '‡§¨‡§π‡•Ä', '‡§â‡§∏‡•Ä', '‡§Æ‡§ó‡§∞', '‡§ï‡§∞', '‡§Æ‡•á', '‡§è‡§∏', '‡§â‡§®', '‡§∏‡•ã', '‡§Ö‡§§', '‡§π‡•Ç‡§Ç', '‡§®']\n",
        "nwords = []\n",
        "for t in texts:\n",
        "    temp = t.split(\"\\n\")\n",
        "    nsen.append(len(temp))\n",
        "    nwords.append(np.sum([len(sent.split()) for sent in temp]))\n",
        "    sentences.extend(temp)\n",
        "sentences = [text_normalize(line.strip(), \"hi\") for line in sentences]\n",
        "words = []\n",
        "for s in sentences:\n",
        "    words.extend(s.split())\n",
        "\n",
        "print(\"Total number of words: \", str(len(words)))\n",
        "print(\"Total number of characters: \", str(len(\" \".join(words))))\n",
        "print(\"Vocab Size: \", str(len(np.unique(words))))\n",
        "print(\"Mean sentences per audio file: \", str(np.mean(nsen)))\n",
        "print(\"Mean words per audio file: \", str(np.mean(nwords)))"
      ],
      "metadata": {
        "id": "BEKZkH7Laj9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filter_words = []\n",
        "for w in words:\n",
        "    if w not in stop_words:\n",
        "        filter_words.append(w)\n",
        "print(\"Top 10 highest repeating words: \\n\")\n",
        "pd.Series(filter_words).value_counts().head(10)"
      ],
      "metadata": {
        "id": "QV9E00Yb7FCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_texts = []\n",
        "for t in texts:\n",
        "    new_texts.append(text_normalize(texts[0].replace('\\n', ' '), \"hi\"))\n",
        "\n",
        "tokenized = []\n",
        "for t in new_texts:\n",
        "    tokenized.extend(word_tokenize(t))\n",
        "\n",
        "def generate_non_overlapping_ngrams(words, n):\n",
        "    return [\" \".join(words[i:i+n]) for i in range(0, len(words), n) if len(words[i:i+n]) == n]\n",
        "\n",
        "\n",
        "gram3 = generate_non_overlapping_ngrams(tokenized, 3)\n",
        "gram4 = generate_non_overlapping_ngrams(tokenized, 4)\n",
        "\n",
        "gram3_counts = Counter(gram3)\n",
        "gram4_counts = Counter(gram4)\n",
        "\n",
        "print(\"Most common 3-grams:\", gram3_counts.most_common(5))\n",
        "print(\"Most common 4-grams:\", gram4_counts.most_common(5))"
      ],
      "metadata": {
        "id": "RXUqBjupk9c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4.7: Text Bias Analysis"
      ],
      "metadata": {
        "id": "CcYFBFKQakTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the scraped data is from one source only, it is important to perform bias analysis in our dataset for generalizability evaluation. To identify biased words, I compare token frequency of word from a general purpose hindi corpus. If token frequency of word is more than double the token frequency from a general purpose corpus then word seems to be biased."
      ],
      "metadata": {
        "id": "QhZRyezNk_Ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#OSCAR small dataset\n",
        "cc_hi = load_dataset(\"nthngdy/oscar-small\", language=\"hi\")\n",
        "cc_hi = cc_hi['train']['text']\n",
        "cc_texts = []\n",
        "for t in tqdm(cc_hi):\n",
        "    temp = text_normalize(t.replace('\\n', ' '), \"hi\")\n",
        "    temp = temp.split()\n",
        "    cc_texts.extend(temp)\n",
        "\n",
        "cc_vc = dict(pd.Series(cc_texts).value_counts())\n",
        "current_vc = dict(pd.Series(tokenized).value_counts())\n",
        "cc_sum = sum(list(cc_vc.values()))\n",
        "current_sum = sum(list(current_vc.values()))\n",
        "\n",
        "subset = []\n",
        "for key in list(current_vc.keys()):\n",
        "    if key in stop_words:\n",
        "        continue\n",
        "    try:\n",
        "        if(current_vc[key]/current_sum < 2*cc_vc[key]/cc_sum):\n",
        "            continue\n",
        "    except:\n",
        "        pass\n",
        "    subset.append(key)\n",
        "print(subset[:10])"
      ],
      "metadata": {
        "id": "IFbRt4KFakuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above subset was then keyed into GPT-4 for categorization into names, verbs, nouns, etc."
      ],
      "metadata": {
        "id": "ym_lzDeLlDP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bias_analysis = {\n",
        "    \"Names\": ['‡§Æ‡§∞‡§ø‡§Ø‡§Æ', '‡§¶‡§æ‡§µ‡•Ä‡§¶', '‡§Ø‡•á‡§∂‡•Å', '‡§Ø‡•ã‡§∏‡•á‡§´‡§º', '‡§Æ‡§∏‡•Ä‡§π', '‡§Ø‡§æ‡§ï‡•ã‡§¨', '‡§Ö‡§¨‡•ç‡§∞‡§æ‡§π‡§æ‡§Æ', '‡§Ø‡§π‡•Ç‡§¶‡§æ‡§π', '‡§á‡§Æ‡•ç‡§Æ‡§æ‡§®‡•Å‡§è‡§≤',\n",
        "              '‡§â‡§ú‡•ç‡§ú‡§ø‡§Ø‡§æ‡§π', '‡§Ø‡•ã‡§•‡§æ‡§Æ', '‡§Ü‡§ñ‡§º‡§æ‡§ú‡§º', '‡§π‡•á‡§ú‡§º‡•á‡§ï‡§ø‡§Ø‡§æ', '‡§Æ‡§®‡§∂‡•ç‡§∂‡•á‡§π', '‡§Ö‡§Æ‡•ã‡§®', '‡§Ø‡•ã‡§∂‡§ø‡§Ø‡§æ‡§π', '‡§Ø‡§ñ‡§º‡•ã‡§®‡§ø‡§Ø‡§æ',\n",
        "              '‡§∏‡§≤‡§æ‡§•‡§ø‡§è‡§≤', '‡§ú‡§º‡•á‡§∞‡•ã‡§¨‡§æ‡§¨‡•á‡§≤', '‡§Ö‡§¨‡§ø‡§π‡•Ç‡§¶', '‡§è‡§≤‡§ø‡§Ø‡§æ‡§ï‡§ø‡§Æ', '‡§Ü‡§ú‡§º‡•ã‡§∞', '‡§∏‡§æ‡§¶‡•ã‡§ï', '‡§Ü‡§ñ‡§º‡§ø‡§Æ', '‡§è‡§≤‡§ø‡§π‡•Ç‡§¶',\n",
        "              '‡§è‡§≤‡§ø‡§Ø‡§æ‡§ú‡§º‡§∞', '‡§Æ‡§§‡•ç‡§•‡§æ‡§®', '‡§∂‡§≤‡•ã‡§Æ‡•ã‡§®', '‡§∞‡•ã‡§¨‡•ã‡§Ü‡§Æ', '‡§π‡§¨‡•Ä‡§Ø‡§æ', '‡§Ü‡§∏‡§´', '‡§Ø‡§π‡•ã‡§∂‡§æ‡§´‡§º‡§æ‡§§', '‡§Ø‡•ã‡§∞‡§æ‡§Æ', '‡§∏‡§≤‡§Æ‡•ã‡§®',\n",
        "              '‡§¨‡•ã‡§Ö‡§ú‡§º', '‡§ì‡§¨‡•á‡§¶', '‡§Ø‡§ø‡§∂‡•à', '‡§´‡§º‡§æ‡§∞‡•á‡§∏', '‡§π‡•á‡§ú‡§º‡§∞‡•ã‡§®', '‡§π‡§æ‡§∞‡§æ‡§Æ', '‡§Ö‡§Æ‡•ç‡§Æ‡•Ä‡§®‡§æ‡§¶‡§æ‡§¨', '‡§®‡§æ‡§π‡§∂‡•ç‡§∂‡•ã‡§®', '‡§Ø‡§ø‡§§‡•ç‡§∏‡§π‡§æ‡§ï',\n",
        "              '‡§§‡§æ‡§Æ‡§æ‡§∞', '‡§∞‡§æ‡§π‡§æ‡§¨', '‡§∞‡•Ç‡§•', '‡§â‡§∞‡§ø‡§Ø‡§æ‡§π', '‡§ú‡§º‡§æ‡§∞‡§æ', '‡§™‡•ç‡§∞‡§≠‡•Å', '‡§™‡§∞‡§Æ‡•á‡§∂‡•ç‡§µ‡§∞'],\n",
        "    \"Verbs\": ['‡§â‡§§‡•ç‡§™‡§®‡•ç‡§®', '‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•á', '‡§∞‡§ñ‡§æ', '‡§¶‡•á‡§ó‡•Ä', '‡§™‡•Å‡§ï‡§æ‡§∞‡•á‡§ó‡•Ä', '‡§ö‡•Å‡§ï‡§æ', '‡§Æ‡§æ‡§≤‡•Ç‡§Æ', '‡§ó‡§∞‡•ç‡§≠‡§µ‡§§‡•Ä', '‡§§‡§Ø',\n",
        "              '‡§ö‡§æ‡§π‡§§‡•á', '‡§π‡•Å‡§à‡§Ç', '‡§™‡§°‡§º‡•á', '‡§§‡•ç‡§Ø‡§æ‡§ó', '‡§¶‡•á‡§®‡•á', '‡§®‡§ø‡§∞‡•ç‡§£‡§Ø', '‡§®‡§ø‡§∂‡•ç‡§ö‡§Ø', '‡§ú‡§æ‡§ó‡§®‡•á', '‡§∞‡§ñ‡§®‡§æ', '‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞',\n",
        "              '‡§¶‡•á‡§ó‡•Ä', '‡§ï‡§∞‡•á‡§ó‡•Ä', '‡§¶‡•á‡§Ç‡§ó‡•á'],\n",
        "    \"Nouns\": ['‡§ú‡§®‡•ç‡§Æ', '‡§™‡•Å‡§§‡•ç‡§∞', '‡§™‡§§‡•ç‡§®‡•Ä', '‡§®‡§æ‡§Æ', '‡§≠‡§æ‡§à', '‡§µ‡§Ç‡§∂‡§ú', '‡§∏‡•ç‡§µ‡§∞‡•ç‡§ó‡§¶‡•Ç‡§§', '‡§µ‡§ø‡§µ‡§æ‡§π', '‡§™‡•Ä‡§¢‡§º‡§ø‡§Ø‡§æ‡§Ç',\n",
        "              '‡§™‡•ç‡§∞‡§ï‡§æ‡§∞', '‡§ó‡§∞‡•ç‡§≠', '‡§∏‡§Ç‡§§‡§æ‡§®', '‡§ï‡•Å‡§Ç‡§µ‡§æ‡§∞‡•Ä', '‡§ï‡§®‡•ç‡§Ø‡§æ', '‡§ó‡§∞‡•ç‡§≠‡§ß‡§æ‡§∞‡§£', '‡§™‡§æ‡§™‡•ã‡§Ç', '‡§â‡§¶‡•ç‡§ß‡§æ‡§∞', '‡§∏‡§¨',\n",
        "              '‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø‡§µ‡§ï‡•ç‡§§‡§æ', '‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ', '‡§µ‡§ö‡§®', '‡§Ö‡§∞‡•ç‡§•', '‡§π‡§Æ‡§æ‡§∞‡•á', '‡§µ‡§Ç‡§∂‡§æ‡§µ‡§≤‡•Ä'],\n",
        "    \"Adjectives\": ['‡§™‡§µ‡§ø‡§§‡•ç‡§∞', '‡§ß‡§∞‡•ç‡§Æ‡•Ä', '‡§™‡§µ‡§ø‡§§‡•ç‡§∞‡§æ‡§§‡•ç‡§Æ‡§æ', '‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§', '‡§ß‡§∞‡•ç‡§Æ‡•Ä'],\n",
        "    \"Others\": ['‡§â‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á', '‡§ï‡•ç‡§Ø‡•ã‡§Ç', '‡§ï‡§ø‡§Ç‡§§‡•Å', '‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø', '‡§á‡§∏‡§∏‡•á', '‡§â‡§®‡§ï‡•á', '‡§ú‡§ø‡§®‡§ï‡•á', '‡§ú‡§ø‡§®‡§Æ‡•á‡§Ç', '‡§â‡§∏‡§∏‡•á',\n",
        "               '‡§â‡§®‡§™‡§∞', '‡§Æ‡§§', '‡§á‡§∏‡§≤‡§ø‡§Ø‡•á', '‡§µ‡•à‡§∏‡§æ']\n",
        "}"
      ],
      "metadata": {
        "id": "Vlapa2bClFSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above, it can be seen that most names are from christianity due to the source of the dataset, hence it is important to include a diversity of religious data sources to prevent the model to be biased to a certain religion."
      ],
      "metadata": {
        "id": "XJ95v8lXlG3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4.8: Phoneme-Level Analysis"
      ],
      "metadata": {
        "id": "TE19kzjnalIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_words = []\n",
        "for keys in list(alignment.keys()):\n",
        "    for word in alignment[keys]:\n",
        "        if word['word_en']!=\"***\":\n",
        "            hindi_words.append(word['word_hi'])\n",
        "\n",
        "def phonemize_text(text_list):\n",
        "    backend = EspeakBackend(language='hi', punctuation_marks=';.', preserve_punctuation=False)\n",
        "    separator = Separator(phone=',', word='')\n",
        "    phonemes = backend.phonemize(text_list, separator=separator)\n",
        "    return phonemes\n",
        "\n",
        "phonemes = phonemize_text(hindi_words)\n",
        "\n",
        "phoneme_list = ' '.join(phonemes).split(',')\n",
        "phoneme_list = [pho.strip() for pho in phoneme_list if pho!='']\n",
        "phoneme_freq = Counter(phoneme_list)\n",
        "\n",
        "df_phoneme = pd.DataFrame(list(phoneme_freq.items()), columns=['Phoneme', 'Frequency'])\n",
        "df_phoneme = df_phoneme.sort_values('Frequency', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.bar(df_phoneme['Phoneme'], df_phoneme['Frequency'], color='skyblue')\n",
        "plt.xlabel('Phonemes')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Phoneme Frequency Distribution')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "sC3KOQNuallK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_phoneme[-20:])"
      ],
      "metadata": {
        "id": "tPAMWPzhlLUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phonemes with low frequency need to be included in the corpus for a better general purpose WER and CER"
      ],
      "metadata": {
        "id": "KA6AMLa7lML3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gram3 = generate_non_overlapping_ngrams(phoneme_list, 3)\n",
        "gram4 = generate_non_overlapping_ngrams(phoneme_list, 4)\n",
        "gram5 = generate_non_overlapping_ngrams(phoneme_list, 5)\n",
        "gram6 = generate_non_overlapping_ngrams(phoneme_list, 6)\n",
        "\n",
        "gram3_counts = Counter(gram3)\n",
        "gram4_counts = Counter(gram4)\n",
        "gram5_counts = Counter(gram5)\n",
        "gram6_counts = Counter(gram6)\n",
        "\n",
        "print(\"Least common 3-grams:\", gram3_counts.most_common()[-5:])\n",
        "print(\"Least common 4-grams:\", gram4_counts.most_common()[-5:])\n",
        "print(\"Least common 5-grams:\", gram5_counts.most_common()[-5:])\n",
        "print(\"Least common 6-grams:\", gram6_counts.most_common()[-5:])"
      ],
      "metadata": {
        "id": "vt-E1xJLlOGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phoneme sequences with low frequency should be added to the dataset"
      ],
      "metadata": {
        "id": "5OQADcEklR9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Conclusion üìÑ"
      ],
      "metadata": {
        "id": "9e_TCY8ZlxPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use this dataset for STT and TTS applications, different post processing mechanisms are required since TTS datasets focus on high-quality, clear audio recordings whereas STT datasets may include a variety of audio qualities, background noises, and different speaking styles to make the model robust.\n",
        "\n",
        "To use this dataset for TTS, prosodic annotations could be added as required to indicate pitch, stress, pause, emphasis & other emotions.\n",
        "\n",
        "For STT, a more diverse range of audio data including noisy and challenging conditions, different dialects, different age groups & genders should be included to make model more robust. Currently, it seems that there is just one male speaker with one dialect in all recordings.\n"
      ],
      "metadata": {
        "id": "Nqhu9Cqtl183"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Data Insights:\n",
        "- First few seconds of almost all audio files are not transcribed. These should be excluded to improve performance.\n",
        "- Many words are transcribed incorrectly in the dataset. It would be worthwhile to run a commercial ASR on the text and compare it with transcriptions to correct errors.\n",
        "- Words with low DTW/high similarity in pitch and general audio indicate that more variants of those words should be added to dataset for improved performance.\n",
        "- There is high standard deviation for word audio duration. This is partly also because of transcription, alignment errors too.\n",
        "- Audio quality seems to be good with low noise levels.\n",
        "- It is recommended to combine this dataset with texts from other religion to make the proabilities of next token predicition bias free from any specific religion.\n",
        "- Phoneme analysis indicates many individual phonemes and phoneme sequences with low frequency. Sentences with these phonemes and phoneme sequences should be added to the dataset to reduce errors on these low-occuring phonemes."
      ],
      "metadata": {
        "id": "btQGB2nRl211"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üôèüôèüôè   Thank you --- ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶ --- ‡Æ®‡Æ©‡Øç‡Æ±‡Æø   üôèüôèüôè‚Äã"
      ],
      "metadata": {
        "id": "LiBDCsfw7YoY"
      }
    }
  ]
}